---
title: "Text_Mining"
author: "Fereshteh Ahmadi, Seraina Felicitas Zimmermann, Michael Etter, Donjet Dzemaili"
format:
  html:
    df-print: kable
    toc: true
    code-tools: true

editor: visual
---

# Review Analysis for Disneyland

## Part 0: Prepossessing

### Environment and Setting

Please set your own virtual environment if needed

```{r}
#| label: choose_environment


reticulate::use_condaenv("huggingfaceR", required = TRUE)


```

### Libraries

```{r}
#| label: load_packages
#| message: false
#| warning: false
#| code-fold: true

# Load required libraries
library(tidyverse)      # Core packages: dplyr, ggplot2, etc.
library(quanteda)       # Text tokenization and feature matrix creation
library(stopwords)      # Multilingual stopword lists
library(tidytext)       # Tidy-friendly text mining tools
library(ggplot2)        # Grammar of graphics
library(scales)         # Axis formatting utilities
library(plotly)         # Interactive ggplot extensions
library(readr)          # Efficient data reading
library(bertopic)       # Python BERTopic interface
library(htmltools)      # Report rendering support
library(umap)           # UMAP for dimensionality reduction
library(reticulate)     # Python-R integration
library(quanteda.textplots)

# Install missing R packages if needed
if (!require("cld2")) install.packages("cld2")
if (!require("hunspell")) install.packages("hunspell")
if (!require("patchwork")) install.packages("patchwork")
if (!require("synthesisr")) install.packages("synthesisr")

# Load helper libraries
library(cld2)         # Language detection
library(hunspell)     # Spelling correction
library(synthesisr)   # Bibliometric tools
library(patchwork)    # Plot composition

# Avoid scientific notation
options(scipen = 999)

# Define required Python packages
py_deps <- c("transformers", "torch", "numpy", "protobuf", "sentencepiece")

# Install missing Python packages
for (pkg in py_deps) {
  if (!reticulate::py_module_available(pkg)) {
    message(glue::glue("Installing missing Python package: {pkg}"))
    reticulate::conda_install("huggingfaceR", pkg, pip = TRUE)
  }
}

options(scipen=999)



```

### Data

This dataset contains user-generated reviews for Disneyland parks in Hong Kong, California, and Paris. It captures various attributes of each review, enabling analysis of visitor feedback across locations and over time. The dataset includes the following columns:

-   Review_ID: A unique identifier assigned to each review.

-   Rating: A numerical score provided by the reviewer, indicating their overall satisfaction.

-   Year_Month: The year and month when the review was posted.

-   Reviewer_Location: The geographical location of the reviewer, as stated in their profile.

-   Review_Text: The full text of the review, expressing the reviewer's experience and opinions.

-   Year: The year when the review was written, extracted from the Year_Month field.

-   Branch: For which branch the review was written.

```{r}
load('Disneyland.rda')
head(reviews, 1)
```

### Data tidying

To prepare the Disneyland review data for text analysis, a series of text preprocessing steps were performed to clean and standardize the review content. The goal of this process is to remove irrelevant information, reduce noise, and create a clean corpus suitable for natural language processing (NLP) tasks such as sentiment analysis, keyword extraction, or topic modeling.

1.  Stopword Loading: Common English stopwords were imported using the `tidytext` package.

2.  Text Cleaning: All reviews were converted to lowercase, and unwanted elements like URLs, HTML tags, punctuation, numbers, and extra spaces were removed. Reviews with missing text were excluded.

3.  Custom Stopwords: A custom list of frequently repeated and non-informative words—such as "disney", "ride", "park", and location names—was added to the stopword list.

4.  Tokenization and Filtering: Reviews were split into individual words (tokens), and all standard and custom stopwords were removed.

5.  Reconstruction: The remaining tokens were reassembled into cleaned review texts for each review ID.

6.  Merging: The cleaned text was merged back with the original dataset, preserving all metadata.

This process ensures that only the most meaningful and relevant words remain, improving the quality and accuracy of downstream text analysis.

```{r}

# Load stopwords
data("stop_words")

# remove symbols 
reviews_clean <- reviews %>%
  filter(!is.na(Review_Text)) %>%
  mutate(Review_ID,
         Review_Text = Review_Text %>%
           str_to_lower() %>%
           str_replace_all("http\\S+|www\\S+", "") %>%
           str_replace_all("<.*?>", "") %>%
           str_replace_all("[^a-z\\s]", " ") %>%
           str_squish()) 

custom_stops <- tibble(
  word = c("disney", "disneyland", "paris", "hong", "kong", "california", "ride", "rides", "park", "parks", "day")
)
all_stopwords <- bind_rows(stop_words, custom_stops)

# Tokenize and remove stopwords
tokens <- reviews_clean %>%
  unnest_tokens(word, Review_Text) %>%
  anti_join(all_stopwords, by = "word")

# Rebuild cleaned text from tokens and join back to full data
reconstructed <- tokens %>%
  group_by(Review_ID) %>%
  summarise(Review_Text_clean = str_c(word, collapse = " "), .groups = "drop")

# Join back to original metadata
reviews_clean <- reviews_clean %>%
  left_join(reconstructed, by = "Review_ID")

```

-   **Multilingual Reviews:**

Before performing text analysis, we first checked whether the reviews were written in multiple languages. Using a language detection function, the language of each review was identified based on its text content:

-   The `detect_language()` function was applied to the `Review_Text` column to assign a detected language code (e.g., "en" for English) to each review.

-   A filter was then applied to identify any reviews not written in English.

The results show us that there are indeed other languages included in the reviews but we can't make sure that those are correctly detected, specially since some of them contain multiple languages in one review. So instead of removing them we will make sure to use language models which are multilingual.

```{r}

# Detect language of each review
reviews$language <- detect_language(reviews$Review_Text)

# See if there are other languages in the data
reviews[reviews$language!= "en", ] %>% 
  select(Review_Text) %>% 
  head(3)


```

-   **Fake or Sponsored Reviews:**

To improve data quality and reduce the influence of spam or generated content, we implemented checks to identify and remove potentially fake or sponsored reviews.

First if we have look at the length of reviews we see that most of them are short, with some very long outliers:

```{r}
# Add length column
reviews_with_length <- reviews %>%
  mutate(length = nchar(Review_Text))

# Histogram (converted to plotly)
hist_plot <- ggplot(reviews_with_length, aes(x = length)) +
  geom_histogram(binwidth = 50, fill = "grey30") +
  labs(x = "Review Length", y = "Count") +
  theme_minimal()

hist_plotly <- ggplotly(hist_plot)

# Boxplot (converted to plotly)
box_plot <- ggplot(reviews_with_length, aes(y = length)) +
  geom_boxplot(fill = "steelblue", outlier.size = 2) +
  labs(y = "Review Length (Characters)", x = "") +
  theme_minimal()

box_plotly <- ggplotly(box_plot)

# Combine side-by-side using subplot
subplot(hist_plotly, box_plotly, nrows = 1, margin = 0.05, shareY = FALSE, titleX = TRUE, titleY = TRUE)
```

Based on what we can see in the interactive boxplot the upper fence is 1734 and since the lower fence would be negative we will keep all shorter reviews but remove everything longer that 1734 characters since thee could be high chance that these are generated, because usually people don't write such long reviews.

```{r}

reviews_clean <- reviews_clean %>%
  mutate(length = nchar(Review_Text)) %>%
  filter(length <= 1734)

```

Next lets see if there are any repeated copy pasted reviews which again is another sign of being fake.

```{r}

reviews_clean %>%
  unnest_tokens(word, Review_Text_clean) %>%
  group_by(Review_ID) %>%
  summarise(seq = paste(word, collapse = " "), .groups = "drop") %>%
  count(seq) %>%
  filter(n > 1) %>%
  inner_join(reviews_clean, by = c("seq" = "Review_Text_clean")) %>%
  arrange(seq) %>% 
  select(Review_Text) %>% 
  head(4)
    
```

Since we caught some repeated reviews with different ID's we will also remove these.

```{r}

# Generate token sequence per review while keeping all columns
unique_ids <- reviews_clean %>%
  unnest_tokens(word, Review_Text_clean) %>%
  group_by(Review_ID) %>%
  mutate(seq = paste(word, collapse = " ")) %>%
  ungroup() %>%
  distinct(Review_ID, .keep_all = TRUE) %>%
  add_count(seq) %>%
  filter(n == 1) %>%     # keep only non-duplicate content
  pull(Review_ID)

# Step 2: Filter the original data (keep all original columns)
reviews_clean <- reviews_clean %>%
  filter(Review_ID %in% unique_ids)

```

-   **Spelling Mistakes:**

To solve this problem we mainly use NLPs that are capable of context understanding.

## Part 1: Overview and problem identification

🔷 What can we tell about the customers that write reviews?

If we have a look at the average ratings of the reviews we notice that Disneyland Paris has the lowest average rating:

```{r}
reviews %>%
  group_by(Branch) %>%
  summarise(avg_rating = mean(Rating, na.rm = TRUE))
```

To understand the evolution of customer engagement over time, we visualized the number of reviews submitted each year across the three Disneyland branches: California, Hong Kong, and Paris.

```{r}

# Extract year
reviews$year <- year(reviews$Year)

# Plot: Reviews by year and branch
reviews %>%
  count(year, Branch) %>%
  ggplot(aes(x = year, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Year and Branch",
       x = "Year", y = "Number of Reviews") +
  theme_minimal()

```

The bar chart shows review counts by year and park:

-   California consistently leads in the number of reviews, peaking in 2015 with over 3,000 reviews.

-   Paris and Hong Kong also show increasing review activity until around 2015, with Paris slightly ahead of Hong Kong overall.

-   After 2016, all three parks experienced a gradual decline in review volume, which may be attributed to external factors like shifts in travel behavior, changing review habits, or global events.

*What this Chart also shows us is that although Paris has the lowest ratings, it doesn't have the smallest number of visitors who left a review. Up unitl 2019 it still had more visitors than Hong Kong, which shows that this branch is important and the visitors who left a review, had something to say and that they, expect improvements.*

To see who is writing the reviews, we categorized reviewers based on their location relative to each park:

-   Local: The reviewer is from the same country as the park.

-   Tourist: The reviewer is from a different country.

We matched each park with its corresponding country and compared it to the reviewer’s location to assign a Visitor_Type. The summary table shows the distribution of local and tourist reviewers for each Disneyland branch:

```{r}
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join to get park country
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  tidyr::pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(Total = Tourist + Local,
         Tourist_Ratio = Tourist / Total)

```

```{r}
#| echo: false

# Branch-country mapping
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join and classify visitor type
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

# Summarize and reshape data
summary_data <- reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(
    Total = Tourist + Local,
    Tourist_Ratio = Tourist / Total
  )

# Reshape for plotting
plot_data <- summary_data %>%
  pivot_longer(cols = c("Local", "Tourist"), names_to = "Visitor_Type", values_to = "Count")

# Plot
ggplot(plot_data, aes(x = Branch, y = Count, fill = Visitor_Type)) +
  geom_bar(stat = "identity") +
  geom_text(
    data = summary_data,
    aes(x = Branch, y = Total + 10, label = scales::percent(Tourist_Ratio, accuracy = 1)),
    inherit.aes = FALSE
  ) +
  labs(title = "Visitor Composition by Disneyland Branch",
       y = "Number of Visitors",
       x = "Branch") +
  theme_minimal() +
  scale_fill_manual(values = c("Local" = "yellow", "Tourist" = "#FF7415")) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

```

-   Disneyland Paris and Disneyland Hong Kong have overwhelmingly high proportions of tourist reviewers, with tourist ratios of 98.5% and 94.6%, respectively.

-   Disneyland California, by contrast, has a significantly higher share of local reviewers, with a tourist ratio of 36.5%.

This suggests that:

-   The Paris and Hong Kong parks attract primarily international visitors.

-   The California park with a strong local customer base likely due to its regional popularity and better experience, so that they tend to go back.

Understanding the local-vs-tourist split is important for interpreting review content and tailoring park communication or services to different visitor types.

## Part 2: Sentiment analysis

🔷 What do the visitors talk about in their reviews and how does it relate sentiment/ratings?

To answer this question, we applied sentiment analysis using a pre-trained model designed for multilingual text:\
Model: `cardiffnlp/twitter-xlm-roberta-base-sentiment` (via HuggingFace Transformers)

#### Procedure:

1.  Sampling:\
    A random sample of 10,000 cleaned reviews was selected to ensure efficient yet representative processing.

2.  Truncation:\
    Each review was shortened to approximately 450 characters (\~512 tokens after tokenization) to meet model input constraints while retaining meaningful content.

3.  Sentiment Classification:\
    Using the HuggingFace Transformers pipeline, each review was passed through the `cardiffnlp/twitter-xlm-roberta-base-sentiment model`. This model was pre-trained on multilingual Twitter data and classifies text into one of three sentiment categories: positive, neutral, or negative. Although twitter data is not the same as review data, we mainly care about how people feel and since this is one of the best multilingual models, that were listed for sentiment analysis, but we need to be aware of biases that might potentially occur.

Labeling: The model’s sentiment predictions were retained as-is and used to classify the tone of each review:

-   **negative → NEGATIVE**

-   **neutral → NEUTRAL**

-   **positive → POSITIVE**

This approach enabled sentiment classification with a focus on emotional tone and informal language, aligning well with the style and diversity of multilingual user reviews.

```{r}
set.seed(1)

# Shuffle and sample
sampled_reviews <- reviews_tagged %>%
  filter(!is.na(Review_Text_clean)) %>%
  slice_sample(n = 10000) %>%
  mutate(Review_Text_clean = str_trunc(Review_Text_clean, 450))

# Load transformers and set Python seed
transformers <- import("transformers")
reticulate::py_run_string("
import random
import numpy as np
import torch

random.seed(1)
np.random.seed(1)
torch.manual_seed(1)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(1)
")


Sys.setenv(TOKENIZERS_PARALLELISM = "true")

# Load sentiment pipeline for tweet model
sentiment_model <- transformers$pipeline(
  task = "sentiment-analysis",
  model = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
)

# Run predictions
emo_results <- sampled_reviews %>%
  mutate(
    sentiment = map(Review_Text_clean, safely(sentiment_model))
  ) %>%
  mutate(
    sentiment_result = map(sentiment, ~ .x$result[[1]]),
    label = map_chr(sentiment_result, ~ toupper(.x$label))  # POSITIVE, NEUTRAL, NEGATIVE
  )

Sys.setenv(TOKENIZERS_PARALLELISM = "false")
 
```

### Sentiment by Visitor Type and Park Branch

The chart bellow breaks down review sentiment by **visitor type (Local vs. Tourist)** for each Disneyland location. It reveals an important pattern:

```{r}

sentiment_by_visitor <- emo_results %>%
  group_by(Branch, Visitor_Type, label) %>%
  summarise(count = n(), .groups = "drop")


sentiment_normalized <- sentiment_by_visitor %>%
  group_by(Branch, Visitor_Type) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()


ggplot(sentiment_normalized, aes(x = Visitor_Type, y = percent, fill = label)) +
  geom_col(position = "dodge") +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment (%) by Visitor Type and Branch",
    x = "Visitor Type",
    y = "Percentage of Reviews",
    fill = "Sentiment"
  ) +
  theme_minimal()



```

#### Disneyland Paris:

-   Has the highest share of negative reviews, especially from locals (over 35% negative).

-   Positive reviews from tourists are lower than in California or Hong Kong.

#### Disneyland California:

-   Both locals and tourists show strong positive sentiment, with close to 60% of reviews classified as positive.

-   Negative sentiment is notably lower among tourists here than in Paris.

#### Disneyland Hong Kong:

-   Sentiment is relatively balanced and positive for both locals and tourists.

-   Locals are mostly neutral here.

## Part 3: Content analysis

To better understand the content of negative feedback at Disneyland Paris, we generated a word cloud from the most frequently used words in negative sentiment reviews. Common branding terms like “Disneyland,” “Disney,” “park,” and “day” were removed to focus on specific experiences.

```{r}

#| label: wordcloud

custom_stopwords <- c("disneyland", "disney", "park", "day")

# disneyland Paris
paris_tokens <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label=="NEGATIVE", !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean) %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(pattern = custom_stopwords)


paris_dfm <- dfm(paris_tokens)
paris_dfm <- paris_dfm[ntoken(paris_dfm) > 0, ]

textplot_wordcloud(
  paris_dfm,
  min_count = 10,
  max_words = 100,
  color = "purple",
  min_size = 0.5,
  max_size = 4
)


```

#### Key Themes from the Word Cloud:

-   Time and waiting:\
    Words like *“time,” “minutes,” “queue,” “queues,” “wait,” “hours,” “waiting,” “long,” “closed”* indicate visitor frustration with long wait times, ride closures, and inefficient time use.

-   Food and cost:\
    Words such as *“food,” “expensive,” “money,” “tickets,” “cost”* suggest concerns about high prices and poor value for money.

-   Service and staff:\
    The prominence of *“staff,” “rude,” “service,” “hotel,” “restaurant”* points to inconsistent service quality and possible hospitality issues, especially in hotels and dining.

-   Children and families:\
    Frequent mentions of *“kids,” “children,” “family,” “characters”* reflect expectations around family-friendly experiences—possibly unmet for many visitors.

-   Crowds and space:\
    Words like *“people,” “walking,” “line,” “parade”* indicate overcrowding or logistical challenges in navigating the park.

### Topic Modeling

To go beyond individual words and uncover deeper patterns in the complaints, we applied BERTopic, a transformer-based topic modeling algorithm. This technique groups reviews into coherent topics based on semantic similarity, using a multilingual sentence embedding model to understand context and nuance across languages.

#### Model Setup:

-   Model: `BERTopic` with `paraphrase-multilingual-MiniLM-L12-v2` embedding

-   Data: 1,000 randomly sampled negative sentiment reviews from Disneyland Paris

-   Number of topics: Up to 6

```{r}

# Import necessary Python packages
umap <- import("umap")
hdbscan <- import("hdbscan")

set.seed(1)

reticulate::py_run_string("
import random
import numpy as np

random.seed(1)
np.random.seed(1)
")
# Define UMAP with fixed seed
umap_model <- umap$UMAP(
  n_neighbors = 20L,
  n_components = 5L,
  min_dist = 0.0,
  metric = "cosine",
  random_state = 1L
)

# Define HDBSCAN with fixed seed
hdbscan_model <- hdbscan$HDBSCAN(
  min_cluster_size = 10L,
  metric = "euclidean",
  cluster_selection_method = "eom",
  prediction_data = TRUE
)

# Load Python modules
bert <- import("bertopic")
sentence_transformers <- import("sentence_transformers")

# Embedding model (multilingual)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# Initialize BERTopic with fixed UMAP + HDBSCAN
topic_model <- bert$BERTopic(
  embedding_model = embedding_model,
  umap_model = umap_model,
  hdbscan_model = hdbscan_model,
  nr_topics = r_to_py(7L),
  calculate_probabilities = TRUE,
  verbose = TRUE
)

docs <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label == "NEGATIVE", !is.na(Review_Text_clean)) %>%
  slice_sample(n = 1000) %>%
  pull(Review_Text_clean)

# Fit model (run this in console, not inside Rmd chunk) it works for me though
topic_results <- topic_model$fit_transform(docs)

# Get summary of topics
topic_model$get_topic_info() %>% 
  select(Topic, Count, Name)






   
```

Now that we have actual clusters of topics using BERTopic, the interpretation becomes more structured and insightful — because it groups entire reviews based on shared meanings, not just word frequency.

🔹 Topic 0: Delays, Closures, and Family Frustration
This dominant topic combines complaints about long wait times, closed attractions, and disappointing food service — often with direct impact on families with children. These reviews describe experiences where families waited hours for rides or food, only to find attractions closed or service too slow to manage with kids.

🔹 Topic 1: Smoking in the Park
This topic captures growing concerns about smoking policies. Guests complain about smokers in crowded areas, and some even note staff members smoking, which breaks immersion and creates discomfort.

🔹 Topic 2: Timing Issues Around Fireworks and Food
Some visitors mention fireworks that were, rescheduled, or poorly timed with restaurant hours — causing logistical stress, especially in the evening.

🔹 Topic 3: Rude Staff and Cultural Tension
This cluster is centered on reviews that mention rude or unhelpful staff, sometimes with a cultural tone — for example, calling out French staff or comparing with experiences in other parks.

🔹 Topic 4: Disappointment Compared to Florida
A niche but emotionally strong topic: visitors comparing Paris unfavorably to Disney Florida, saying it lacked the same magic, hospitality, or smooth operations.

🔹 Topic 5: Sanitation and Toilets
Guests voice frustration about dirty or smelly toilets, especially when tied to the high cost of entry — making them feel that basic standards weren't met.

And What About Topic -1?
A large number of reviews fall into Topic -1, which BERTopic couldn't cluster clearly — but when we look at the most frequent words, they echo the same core complaints as Topic 0: queues, food, and staff issues. These reviews likely express similar frustration but in less consistent or more emotional language.



### Frequency of Topics in Negative Disneyland Paris Reviews

Using **BERTopic**, we extracted and categorized common themes and removed topic -1. The chart bellow visualizes the frequency of each topic, based on semantic clustering of similar reviews.

```{r}

topic_info <- topic_model$get_topic_info()
topic_info_df <- py_to_r(topic_info)




topic_info_df %>%
  filter(Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in Negative Disneyland Paris Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()


```


### Visualizing Topic Clusters with UMAP

To see the distribution of clusters, we applied UMAP (Uniform Manifold Approximation and Projection) to reduce the high-dimensional text embedding into two dimensions. Each point represents a single review, and colors indicate the topic assigned by the BERTopic model.

```{r}

embeddings <- as.data.frame(py_to_r(topic_model$umap_model$embedding_)) 

colnames(embeddings) <- c("UMAP1", "UMAP2")
topics <- reticulate::py_to_r(topic_results[[1]])
embeddings$topics <- as.factor(topics)
# Remove NA-named columns
embeddings <- embeddings[, !is.na(names(embeddings)) & names(embeddings) != ""]

# Now safe to filter
embeddings <- embeddings %>% filter(topics != -1)


# trimming of outliers
ggplotly(
  ggplot(embeddings) +
    geom_point(aes(UMAP1, UMAP2, color = topics)) +
    theme_classic()
)


```

#### Key Observations:

The largest cluster encompassed a wide range of general complaints, with common themes involving wait times, food availability, and attraction closures — particularly affecting families with children. This dominant topic reflects the broad operational frustrations most frequently expressed by guests.

Smaller, more tightly defined clusters captured specific concerns, such as cleanliness issues, smoking enforcement, and staff interactions. Several reviews referenced unmet expectations when compared to other Disney parks, suggesting that the perceived quality of service in Paris may fall short of what guests anticipate based on international standards.

### Topic Modeling of Positive Reviews – Disneyland California

To help Disneyland in Paris improve even more lets look at the reasons behind very positive reviews of California

```{r}

# Use same UMAP, HDBSCAN, and embedding_model as before

# Filter for positive reviews from Disneyland California
docs_california_pos <- emo_results %>%
  filter(Branch == "Disneyland_California", label == "POSITIVE", !is.na(Review_Text_clean)) %>%
  slice_sample(n = 1000) %>%
  pull(Review_Text_clean)

# Fit topic model on positive California reviews
topic_model_california <- bert$BERTopic(
  embedding_model = embedding_model,
  umap_model = umap_model,
  hdbscan_model = hdbscan_model,
  nr_topics = r_to_py(7L),
  calculate_probabilities = TRUE,
  verbose = TRUE
)

# Run the model (best in console)
topic_results_california <- topic_model_california$fit_transform(docs_california_pos)

# View topic summary
topic_model_california$get_topic_info() %>% 
  select(Topic, Count, Name)




```

Why This Matters

This breakdown confirms that Disneyland California’s positive reviews are rich and diverse, with a strong focus on:

-   Efficient logistics (fast passes, wait times)

-   Family experiences and kid-friendly environments

-   Special events like Halloween and Christmas

-   Entertainment value (fireworks, parades)

-   Cleanliness and staff friendliness

These themes are notably absent or negatively framed in reviews of Disneyland Paris — especially regarding staff and time efficiency — reinforcing our earlier findings.

## Part 4: Locations analysis

🔷 What differences can we detect for the three different locations and are there any interesting trends over time?

To explore how visitor feedback varies over time, we analyzed monthly review counts for each Disneyland location from 2010 to 2019. The line chart bellow shows review volume by month, year, and branch (California, Hong Kong, and Paris).

```{r}
reviews <- reviews %>%
  mutate(
    Year_Month = as_date(Year_Month),
    month = month(Year_Month, label = TRUE, abbr = TRUE),
    year = year(Year_Month)
  )

monthly_reviews <- reviews %>%
  filter(!is.na(month) & !is.na(year)) %>%
  count(year, month, Branch)

# Sort month correctly
monthly_reviews$month <- factor(monthly_reviews$month, 
                                 levels = month.abb, 
                                 ordered = TRUE)

# Line plot with facets stacked in 3 rows
ggplot(monthly_reviews, aes(x = month, y = n, color = as.factor(year), group = year)) +
  geom_line(size = 1.2) +
  facet_wrap(~ Branch, ncol = 1) +
  labs(
    title = "Monthly Review Trends per Year and Branch",
    x = "Month", y = "Number of Reviews", color = "Year"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9),
    strip.text = element_text(size = 12, face = "bold")
  )



```

Key Observations:

Seasonality across all parks:

-   All three parks show consistent peaks in review activity from May to August, aligning with summer holidays in many countries.

-   There's often a secondary rise in December, likely due to holiday events and winter travel.

Disneyland California:

-   The most active park in terms of reviews overall.

-   Clear seasonal peaks occur around June to August, especially in 2014–2016.

-   An unusually strong post-summer rebound in December is visible in several years, likely driven by Thanksgiving and Christmas traffic.

-   The review volume dropped somewhat after 2017, which may reflect platform or behavior changes rather than actual visitation.

Disneyland Hong Kong:

-   Reviews peaked in spring and early summer (April–July) and in December.

-   Fluctuations are more moderate, and volumes are lower than California, but follow a similar summer-dominated pattern.

-   Notable review growth in 2016, then slight decline afterward.

Disneyland Paris:

-   Seasonal trends, with major spikes in July and August each year.

-   This suggests a strong reliance on European summer tourism, particularly school holiday travelers.

-   Activity decreases sharply in September and remains low through the fall, with less December review activity compared to California.

Why These Peaks Happen

-   Summer (June–August): School vacations, family travel, and warm weather drive the bulk of visitation.

-   December: Holiday events (e.g., Christmas parades, themed décor) often lead to increased reviews, particularly in California.

-   April and October spikes (in some years): Often coincide with spring breaks and Halloween events, especially in California and Hong Kong.

This analysis shows that while all parks follow seasonal review trends, they vary in intensity and consistency:

-   California leads in volume and exhibits both summer and winter peaks.

-   Paris is highly summer-dependent, with less consistent winter activity,but positive reviews from California showed us that people really like chrismas events in Disneyland California.

-   Hong Kong has a bit of a different seasonality with more people around spring but also winter, and that more than Paris.

### Average Rating Trends Over Time by Location

The line plot shows the average visitor rating (1–5) for each Disneyland location from 2010 to 2019. This highlights how guest satisfaction has changed over time and varies between the three parks.

```{r}
reviews %>%
  group_by(Year, Branch) %>%
  summarise(avg_rating = mean(Rating, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = avg_rating, color = Branch)) +
  geom_line(size = 1.2) +
  labs(title = "Average Rating Over the Years by Location",
       x = "Year",
       y = "Average Rating") +
  theme_minimal()


```

Key Insights:

🟥 Disneyland California (Red) Starts off with the highest average rating (\~4.5) in 2010.

Gradual decline from 2011 to 2018, with a small rebound in 2019.

Despite the drop, it remains the top-rated park throughout most of the period.

🟩 Disneyland Hong Kong (Green) Begins lower than California but shows a sharp increase from 2010 to 2011.

Ratings stabilize between 4.2–4.3 from 2013 onward.

Slight decline after 2018, but it overtakes California in 2018 for the first time.

🟦 Disneyland Paris (Blue) Starts with the lowest ratings, around 3.8 in 2010.

Shows slow but steady growth until 2016, peaking near 4.1.

From 2017 onward, there’s a significant drop, reaching the lowest point (\~3.7) by 2019.

### Sentiment Trend Over the Years by Branch

This chart tracks the percentage of reviews classified as positive, neutral, or negative each year from 2010 to 2019 for Disneyland California, Hong Kong, and Paris. It provides a clear look at how public perception of each park has changed over time.

```{r}
sentiment_by_year <- emo_results %>%
  group_by(Branch, Year, label) %>%
  summarise(count = n(), .groups = "drop")

sentiment_normalized_yearly <- sentiment_by_year %>%
  group_by(Branch, Year) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()

ggplot(sentiment_normalized_yearly, aes(x = Year, y = percent, color = label)) +
  geom_line(size = 1.2) +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment Trend Over the Years by Branch",
    x = "Year",
    y = "Percentage of Reviews",
    color = "Sentiment"
  ) +
  theme_minimal()
```

**Disneyland California**\
Positive sentiment remains consistently strong throughout the decade, hovering around 50-40%. Negative sentiment shows a gradual increase over time, but still remains lower than both positive and neutral sentiment. Neutral sentiment stays fairly stable across the years.

**Disneyland Hong Kong**\
Positive sentiment seems to go down from 2017-2018 and in contrast neutral sentiment goes up. There's a dramatic spike in neutral sentiment in 2019, suggesting a possible lower satisfaction in guest experience or operations. Negative sentiment remains relatively low and stable after a sharp drop in 2011.

**Disneyland Paris**\
Contrary to the average trend, positive sentiment alone **increased** after 2013 and remained stable or even slightly rising through 2018 although the number of reviews overall has also decreased overtime. Neutral sentiment steadily decreased, while negative sentiment rose modestly over time — but did not surpass positive sentiment. This indicates a **more mixed sentiment trend**, rather than a clear decline but as we can see, the lines are very close to each other in comparison to California and Hong Kong, so the ratios are very similar and the drop of neutral reviews in combination with the rise of negative ones, resulted in an average down going trend.

### Visitor Composition by Disneyland Branch

In Chapter 1, you see a visualization of the local and tourist review distribution (yellow and orange stacked bar chart), which reveals several key findings:\
Where we can see: Key Findings:

-   Disneyland California has the most balanced audience:

    -   \~36% local visitors — significantly more than other parks.

    -   \~64% of reviews come from tourists.

-   Disneyland Hong Kong:

    -   \~95% tourists, just 5% local reviews.

    -   Still, Hong Kong performs relatively well in ratings and sentiment.

<!-- -->

-   Disneyland Paris:
    -   Overwhelmingly tourist-driven, with 99% of reviews from international visitors
    -   This aligns with its consistently lower sentiment and rating performance.

Why This Matters

Local visitors are often:

-   More familiar with the park experience

-   Less critical of occasional shortcomings

-   More likely to visit repeatedly, reducing the pressure for a “perfect” experience

Tourists, on the other hand:

-   May face language or cultural barriers

-   Are often first-time visitors with high expectations

-   Experience the park as a one-time destination, meaning any frustration can lead to more negative reviews

This helps explain the higher sentiment and ratings for Disneyland California and the growing dissatisfaction seen at Disneyland Paris.

Final Insight

The visitor composition is a powerful predictor of review tone. Parks with more local engagement (like California) tend to receive more positive, forgiving, and emotionally rich reviews. Parks relying heavily on tourists (like Paris) face more scrutiny and dissatisfaction — especially when logistical or service-related issues arise.

This makes visitor type a critical consideration for interpreting sentiment, planning service improvements, and targeting future marketing strategies.

## Part 5: Conclusion and Discussion

🔷 What specific advice can we give to the Park management based on our analysis? How can we integrate the analysis of reviews in internal processes, can we think of any data products that would be of value?

Over the course of this analysis, we explored review data from Disneyland California, Paris, and Hong Kong to understand how visitor experiences differ by location, time, and reviewer type. By examining ratings, sentiment trends, thematic topics, and the composition of reviewers, we uncovered actionable insights that can support more informed decision-making and experience design across the parks.

One of the most consistent findings is that Disneyland California performs best in terms of sentiment and average ratings. This park not only receives the most reviews but also sustains a high level of visitor satisfaction over time. A key contributing factor appears to be its balanced visitor base — around 36% of reviews come from locals. These local visitors tend to be more familiar with the park, more forgiving of minor inconveniences, and often more emotionally attached to the Disneyland brand. Their reviews are more positive on average and contribute to a generally stable reputation.

In contrast, Disneyland Paris receives the lowest ratings and most negative sentiment, with a clear downward trend over the last decade. This park is reviewed almost exclusively by tourists (99%). Our sentiment analysis showed that local people give even more negative reviews than tourists, which shows why there aren't many of them. On the other hand, many of whom are first-time visitors with high expectations. When these expectations are not met — due to long queues, unclean facilities, or staff-related frustrations — tourists are also more likely to leave critical reviews. Topic modeling of negative reviews from Paris revealed common issues such as long wait times, unclean toilets, staff rudeness, lack of shows, and a perceived lack of value for money.

From this analysis, it's evident that Disneyland Paris has a significant opportunity to improve the guest experience by directly addressing the topics most associated with negative feedback. Management could consider expanding queue management systems, such as virtual lines or FastPass-like features, to alleviate frustration related to long waits. Moreover, enhancing the entertainment offering — including more frequent fireworks, parades, and seasonal shows — could create the type of magical experiences that guests consistently praise in California.

In addition we saw, that the number of reviews significantly decreased for all branches, which makes the information retrival more difficult and less trustworthy if most of them are old. To continuously improve Disneyland's customer experience, it is important to have updated data and therefore we would encourage Disneyland to gather more reviews and analyse them every year. Since we had difficulty finding a suitable sentiment classifier, we would also suggest them to fine tune a model for their reviews to perfectly classify and analyse them.

In conclusion, visitor reviews are an incredibly rich and underutilized source of insight. They capture authentic guest perspectives at scale and can guide both strategic and tactical improvements. Disneyland California shows what consistent, positive experiences can yield, while Disneyland Paris shows where missed expectations can erode public sentiment over time. By responding directly to review data — especially in Paris — Disney can protect its brand reputation and continue delivering magical experiences that live up to the promise of its parks.
