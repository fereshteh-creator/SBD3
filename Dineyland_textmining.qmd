---
title: "Text_Mining"
author: "Fereshteh Ahmadi, Seraina Felicitas Zimmermann, Michael Etter, Donjet Dzemaili"
format:
  html:
    df-print: paged
    toc: true
    code-tools: true

editor: visual
---

# Review Analysis for Disneyland

## Part 0: Prepossessing

### Environment and Setting

Please set your own virtual environment if needed

```{r}
#| label: choose_environment


reticulate::use_condaenv("huggingfaceR", required = TRUE)
set.seed(19)

```

### Libraries

```{r}
#| label: load_packages
#| message: false
#| warning: false
#| code-fold: true

library(tidyverse)            # Core data wrangling and visualization packages (dplyr, ggplot2, etc.)
library(quanteda)             # Text tokenization, preprocessing, and creating document-feature matrices
library(stopwords)            # Provides multilingual stopword lists for text cleaning
library(topicmodels)          # Topic modeling algorithms like LDA and CTM (Latent Dirichlet Allocation)
library(tidytext)             # Tidy-friendly tools for text mining and sentiment analysis
library(quanteda.textplots)   # Visualization tools for quanteda objects (word clouds, networks, etc.)
library(ggplot2)              # Grammar of graphics plotting system (used by tidyverse)
library(scales)               # Formatting and scaling functions for plots (e.g., percentages, dates)
library(devtools)             # Tools to install and manage R packages from GitHub
library(plotly)               # Interactive plots (works well with ggplot2 for interactivity)
library(readr)                # Efficient functions to read data (like CSVs) ‚Äî part of tidyverse
library(ldatuning)            # Helps choose optimal number of topics in LDA models
library(bertopic)             # (If installed) R wrapper or interface to the Python BERTopic library
library(htmltools)            # Used in HTML report rendering and interactive outputs (like word clouds)
library(umap)                 # Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction
library(reticulate)

if (!require("cld2")) install.packages("cld2")
if (!require("hunspell")) install.packages("hunspell")
library(cld2) # for language detection
library(hunspell) # for spelling correction

options(scipen=999)



```

### Data

This dataset contains user-generated reviews for Disneyland parks in Hong Kong, California, and Paris. It captures various attributes of each review, enabling analysis of visitor feedback across locations and over time. The dataset includes the following columns:

-   review_id: A unique identifier assigned to each review.

-   Rating: A numerical score provided by the reviewer, indicating their overall satisfaction.

-   Year_Month: The year and month when the review was posted.

-   Reviewer Location: The geographical location of the reviewer, as stated in their profile.

-   Year: The year when the review was written, extracted from the Year_Month field.

-   Review Text: The full text of the review, expressing the reviewer's experience and opinions.

-   Language: The language in which the review was written.

```{r}
load('Disneyland.rda')
head(reviews)
```

### Data tidying

To prepare the Disneyland review data for text analysis, a series of text preprocessing steps were performed to clean and standardize the review content. The goal of this process is to remove irrelevant information, reduce noise, and create a clean corpus suitable for natural language processing (NLP) tasks such as sentiment analysis, keyword extraction, or topic modeling.

1.  Stopword Loading: Common English stopwords were imported using the `tidytext` package.

2.  Text Cleaning: All reviews were converted to lowercase, and unwanted elements like URLs, HTML tags, punctuation, numbers, and extra spaces were removed. Reviews with missing text were excluded.

3.  Custom Stopwords: A custom list of frequently repeated and non-informative words‚Äîsuch as "disney", "ride", "park", and location names‚Äîwas added to the stopword list.

4.  Tokenization and Filtering: Reviews were split into individual words (tokens), and all standard and custom stopwords were removed.

5.  Reconstruction: The remaining tokens were reassembled into cleaned review texts for each review ID.

6.  Merging: The cleaned text was merged back with the original dataset, preserving all metadata.

This process ensures that only the most meaningful and relevant words remain, improving the quality and accuracy of downstream text analysis.

```{r}

# Load stopwords
data("stop_words")

# remove symbols 
reviews_clean <- reviews %>%
  filter(!is.na(Review_Text)) %>%
  mutate(Review_ID,
         Review_Text = Review_Text %>%
           str_to_lower() %>%
           str_replace_all("http\\S+|www\\S+", "") %>%
           str_replace_all("<.*?>", "") %>%
           str_replace_all("[^a-z\\s]", " ") %>%
           str_squish()) 

custom_stops <- tibble(
  word = c("disney", "disneyland", "paris", "hong", "kong", "california", "ride", "rides", "park", "parks", "day")
)
all_stopwords <- bind_rows(stop_words, custom_stops)

# Tokenize and remove stopwords
tokens <- reviews_clean %>%
  unnest_tokens(word, Review_Text) %>%
  anti_join(all_stopwords, by = "word")

# Rebuild cleaned text from tokens and join back to full data
reconstructed <- tokens %>%
  group_by(Review_ID) %>%
  summarise(Review_Text_clean = str_c(word, collapse = " "), .groups = "drop")

# Join back to original metadata
reviews_clean <- reviews_clean %>%
  left_join(reconstructed, by = "Review_ID")

```

-   **Multilingual Reviews:**

Before performing text analysis, we first checked whether the reviews were written in multiple languages. Using a language detection function, the language of each review was identified based on its text content:

-   The `detect_language()` function was applied to the `Review_Text` column to assign a detected language code (e.g., "en" for English) to each review.

-   A filter was then applied to identify any reviews not written in English.

This step ensures that only English-language reviews are included in the analysis, avoiding noise or misinterpretation caused by multilingual content. Since its difficult to fully recognize other languages we will use multilingual models.

```{r}

# Detect language of each review
reviews$language <- detect_language(reviews$Review_Text)

# See if there are other languages in the data
reviews[reviews$language!= "en", ]


```

-   **Fake or Sponsored Reviews:**

To improve data quality and reduce the influence of spam or low-effort content, we implemented checks to identify and remove potentially fake or sponsored reviews. The filtering process involved two key steps:

1.  Short Review Removal:\
    Very short reviews (fewer than 10 words) were identified and excluded. These were predominantly 5-star ratings with minimal content, often lacking meaningful information for analysis.

2.  Duplicate Review Removal:\
    Reviews with identical cleaned text (`Review_Text_clean`) were flagged as duplicates, likely indicating copy-pasted or spammed content. Only unique reviews were retained by removing entries that appeared more than once.

This step helps ensure that the final dataset is composed of authentic, original, and content-rich reviews, enhancing the reliability of subsequent text analyses.

```{r}
# very short reviews seem to be mostly 5 star ratings
reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  arrange(word_count) %>%
  select(Rating, Review_Text, word_count) %>% 
  filter(word_count<10)

reviews_clean<-reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  filter(word_count>10) %>% 
  group_by(Review_Text_clean) %>% 
    mutate(duplicate_count = n()) %>%
    ungroup() %>% 
    filter(duplicate_count == 1)


    
```

-   **Spelling Mistakes:**\

To solve this problem we mainly use NLPs.

--\> sie hat was dar√ºber gesagt, aber ich weiss es nicht mehr. M√ºssen wir das noch machen?

## part 1: Content analysis and topic modeling

üî∑ What can we tell about the customers that write reviews?

To begin our content analysis, we examined the length of review texts to understand how much customers typically write. The histogram above shows the distribution of review lengths (in number of characters).

The results reveal that:

-   Most customers write relatively short reviews, with a large concentration under 500 characters.

-   A long tail exists, indicating that while a few reviews are exceptionally long (over 10,000 characters), they are rare.

-   This suggests that the majority of feedback is brief, likely focused on high-level impressions or quick reactions, while only a minority of users provide detailed, in-depth reviews.

Understanding this behavior helps shape our expectations for topic modeling and sentiment analysis‚Äîbrief reviews may yield fewer unique insights per document, while longer reviews may offer richer semantic content.

```{r}
# most of the customers write shorter reviews
reviews %>%
  mutate(length = nchar(Review_Text)) %>%
  ggplot(aes(x = length)) +
  geom_histogram(binwidth = 20)

```

To understand the evolution of customer engagement over time, we visualized the number of reviews submitted each year across the three Disneyland branches: California, Hong Kong, and Paris.

The bar chart shows review counts by year and park:

-   California consistently leads in the number of reviews, peaking in 2015 with over 3,000 reviews.

-   Paris and Hong Kong also show increasing review activity until around 2015, with Paris slightly ahead of Hong Kong overall.

-   After 2016, all three parks experienced a gradual decline in review volume, which may be attributed to external factors like shifts in travel behavior, changing review habits, or global events.

```{r}

# Extract year
reviews$year <- year(reviews$Year)

# Plot: Reviews by year and branch
reviews %>%
  count(year, Branch) %>%
  ggplot(aes(x = year, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Year and Branch",
       x = "Year", y = "Number of Reviews") +
  theme_minimal()

```

To examine how review activity varies throughout the year, we analyzed the distribution of reviews by month for each Disneyland location: California, Hong Kong, and Paris.

The bar chart above displays the number of reviews by month:

-   Review activity increases during spring and summer, particularly from April to August, aligning with typical holiday and travel seasons.

-   July and August are peak months across all three branches, especially in California, suggesting these are high-traffic periods for park visitors.

-   There‚Äôs a secondary rise in November and December, possibly tied to winter holidays and seasonal events at the parks.

-   California receives the highest review volume consistently throughout the year, followed by Paris and then Hong Kong.

```{r}
# Extract month (abbreviated names for labels)
reviews$month <- month(reviews$Year_Month, label = TRUE, abbr = TRUE)

# Plot: Reviews by month and branch
reviews %>%
  count(month, Branch) %>%
  ggplot(aes(x = month, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Month and Branch",
       x = "Month", y = "Number of Reviews") +
  theme_minimal()

```

To better understand who is writing the reviews, we categorized reviewers based on their location relative to each park:

-   Local: The reviewer is from the same country as the park.

-   Tourist: The reviewer is from a different country.

We matched each park with its corresponding country and compared it to the reviewer‚Äôs location to assign a Visitor_Type. The summary table shows the distribution of local and tourist reviewers for each Disneyland branch:

-   Disneyland Paris and Disneyland Hong Kong have overwhelmingly high proportions of tourist reviewers, with tourist ratios of 98.5% and 94.6%, respectively.

-   Disneyland California, by contrast, has a significantly higher share of local reviewers, with a tourist ratio of 36.5%.

This suggests that:

-   The Paris and Hong Kong parks attract primarily international visitors.

-   The California park serves a more balanced mix, with a strong local customer base likely due to its regional popularity and accessibility for U.S. residents.

Understanding the local-vs-tourist split is important for interpreting review content and tailoring park communication or services to different visitor types.

```{r}
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join to get park country
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  tidyr::pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(Total = Tourist + Local,
         Tourist_Ratio = Tourist / Total)

```

## Part 2: Sentiment analysis

üî∑ What do the visitors talk about in their reviews and how does it relate sentiment/ratings?

To understand the emotional tone of visitor reviews, we applied sentiment analysis using a pre-trained BERT model designed for multilingual text:\
Model: `nlptown/bert-base-multilingual-uncased-sentiment` (via HuggingFace Transformers)

#### Procedure:

1.  Sampling:\
    A random sample of 10,000 cleaned reviews was selected to ensure efficient yet representative processing.

2.  Truncation:\
    Each review was shortened to approximately 450 characters (\~512 tokens after tokenization) to meet model input constraints while retaining meaningful content.

3.  Sentiment Classification:\
    Using the HuggingFace Transformers pipeline, each review was passed through a sentiment classifier, which returned a predicted 1‚Äì5 star rating.

4.  Labeling:\
    The numeric star predictions were converted into sentiment labels:

    -   1‚Äì2 stars ‚Üí NEGATIVE

    -   3 stars ‚Üí NEUTRAL

    -   4‚Äì5 stars ‚Üí POSITIVE

This approach allowed us to classify review sentiment with a high degree of linguistic nuance, especially important for a multilingual dataset with short and informal text.

#### Why this matters:

This sentiment layer enables us to:

-   Compare emotional tone across parks and visitor types

-   Correlate sentiment with actual review ratings

-   Explore why Disneyland California may receive better reviews than Paris, especially considering its higher share of local visitors

Next, we will visualize and interpret these sentiment results to uncover key insights

```{r}
set.seed(9) 

# Shuffle and sample the data
sampled_reviews <- reviews_tagged %>%
  filter(!is.na(Review_Text_clean)) %>%
  slice_sample(n = 10000) %>% 
  mutate(Review_Text_clean = str_trunc(Review_Text_clean, 450))  # about 512 tokens after tokenization

# Load Python's transformers package
transformers <- import("transformers")

# Load sentiment analysis pipeline
sentiment_model <- transformers$pipeline(
  task = "text-classification",
  model = "nlptown/bert-base-multilingual-uncased-sentiment"
)


# Enable tokenizer parallelism 
Sys.setenv(TOKENIZERS_PARALLELISM = "true")

emo_results <- sampled_reviews %>%
  mutate(
    sentiment = map(Review_Text_clean, safely(sentiment_model))
  ) %>%
  mutate(
    sentiment_result = map(sentiment, ~ .x$result[[1]]),
    stars = map_dbl(sentiment_result, ~ as.numeric(str_extract(.x$label, "\\d"))),
    label = case_when(
      stars <= 2 ~ "NEGATIVE",
      stars == 3 ~ "NEUTRAL",
      stars >= 4 ~ "POSITIVE"
    )
  )

# Disable parallelism again     
Sys.setenv(TOKENIZERS_PARALLELISM = "false")


 
```

After applying sentiment analysis to a representative sample of reviews, we calculated the average predicted sentiment score (1‚Äì5) for each Disneyland branch:

```{r}

emo_results %>%
  group_by(Branch) %>%
  summarise(avg_sentiment = mean(Rating, na.rm = TRUE))

```

#### Key Observations:

-   Disneyland California has the highest average sentiment, suggesting visitors generally express more positive feelings in their reviews.

-   Disneyland Paris has the lowest sentiment score, indicating a less favorable emotional tone in its reviews, on average.

This supports the initial observation that California receives better reviews, and now we begin to understand how people feel, not just what star ratings they assign.

### Sentiment by Visitor Type and Park Branch

The chart above breaks down review sentiment by **visitor type (Local vs. Tourist)** for each Disneyland location. It reveals an important pattern:

```{r}

sentiment_by_visitor <- emo_results %>%
  group_by(Branch, Visitor_Type, label) %>%
  summarise(count = n(), .groups = "drop")


sentiment_normalized <- sentiment_by_visitor %>%
  group_by(Branch, Visitor_Type) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()


ggplot(sentiment_normalized, aes(x = Visitor_Type, y = percent, fill = label)) +
  geom_col(position = "dodge") +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment (%) by Visitor Type and Branch",
    x = "Visitor Type",
    y = "Percentage of Reviews",
    fill = "Sentiment"
  ) +
  theme_minimal()



```

#### Disneyland Paris:

-   Has the highest share of negative reviews, especially from tourists (over 40% negative).

-   Positive reviews from tourists are significantly lower than in California and Hong Kong.

-   Locals also express relatively low positivity and high negativity, though they represent a very small share of total reviewers.

#### Disneyland California:

-   Both locals and tourists show strong positive sentiment, with over 60% of reviews classified as positive.

-   Negative sentiment is notably lower among tourists here than in Paris.

#### Disneyland Hong Kong:

-   Sentiment is relatively balanced and positive for both locals and tourists.

-   Like California, over 60% of tourist reviews are positive

To better understand the content of negative feedback at Disneyland Paris, we generated a word cloud from the most frequently used words in negative sentiment reviews. Common branding terms like ‚ÄúDisneyland,‚Äù ‚ÄúDisney,‚Äù ‚Äúpark,‚Äù and ‚Äúday‚Äù were removed to focus on specific experiences.

```{r}

#| label: wordcloud

custom_stopwords <- c("disneyland", "disney", "park", "day")

# disneyland Paris
paris_tokens <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label=="NEGATIVE", !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean) %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(pattern = custom_stopwords)


paris_dfm <- dfm(paris_tokens)
paris_dfm <- paris_dfm[ntoken(paris_dfm) > 0, ]

textplot_wordcloud(
  paris_dfm,
  min_count = 10,
  max_words = 100,
  color = "darkblue",
  min_size = 0.5,
  max_size = 4
)


```

#### Key Themes from the Word Cloud:

-   Time and waiting:\
    Words like *‚Äútime,‚Äù ‚Äúminutes,‚Äù ‚Äúqueue,‚Äù ‚Äúqueues,‚Äù ‚Äúwait,‚Äù ‚Äúhours,‚Äù ‚Äúwaiting,‚Äù ‚Äúlong,‚Äù ‚Äúclosed‚Äù* indicate visitor frustration with long wait times, ride closures, and inefficient time use.

-   Food and cost:\
    Words such as *‚Äúfood,‚Äù ‚Äúexpensive,‚Äù ‚Äúmoney,‚Äù ‚Äútickets,‚Äù ‚Äúcost‚Äù* suggest concerns about high prices and poor value for money.

-   Service and staff:\
    The prominence of *‚Äústaff,‚Äù ‚Äúrude,‚Äù ‚Äúservice,‚Äù ‚Äúhotel,‚Äù ‚Äúrestaurant‚Äù* points to inconsistent service quality and possible hospitality issues, especially in hotels and dining.

-   Children and families:\
    Frequent mentions of *‚Äúkids,‚Äù ‚Äúchildren,‚Äù ‚Äúfamily,‚Äù ‚Äúcharacters‚Äù* reflect expectations around family-friendly experiences‚Äîpossibly unmet for many visitors.

-   Crowds and space:\
    Words like *‚Äúpeople,‚Äù ‚Äúcrowded,‚Äù ‚Äúwalking,‚Äù ‚Äúline,‚Äù ‚Äúparade‚Äù* indicate overcrowding or logistical challenges in navigating the park.

### Conclusion:

This word cloud reinforces the findings from the sentiment analysis:

Many negative reviews for Disneyland Paris stem from long queues, high costs, and service quality, particularly from tourist visitors who may have high expectations and limited time.

Combined with the earlier insight that Paris has fewer local reviewers, it becomes clear that managing international guest expectations and experience logistics could be key to improving sentiment.

To go beyond individual words and uncover deeper patterns in the complaints, we applied BERTopic, a transformer-based topic modeling algorithm. This technique groups reviews into coherent topics based on semantic similarity, using a multilingual sentence embedding model to understand context and nuance across languages.

#### Model Setup:

-   Model: `BERTopic` with `paraphrase-multilingual-MiniLM-L12-v2` embedding

-   Data: 1,000 randomly sampled negative sentiment reviews from Disneyland Paris

-   Number of topics: Up to 6 (auto-reduced to 2 dominant topics based on content)

```{r}
# Load Python modules
bert <- import("bertopic")
sentence_transformers <- import("sentence_transformers")

# Embedding model (multilingual)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

topic_model <- bert$BERTopic(
  embedding_model = embedding_model,
  nr_topics = r_to_py(6L),
  calculate_probabilities = TRUE,
  verbose = TRUE
)

docs <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label == "NEGATIVE", !is.na(Review_Text_clean)) %>%
  slice_sample(n = 1000) %>%
  pull(Review_Text_clean)

# Fit model (run this in console, not inside Rmd chunk) it works for me though
topic_results <- topic_model$fit_transform(docs)

# Get summary of topics
topic_model$get_topic_info()






   
```

Key Takeaways:

-   One dominant topic (Topic 1) encompasses the vast majority of reviews, clearly indicating that logistical issues (e.g., waiting, food quality, staff performance) are the biggest source of dissatisfaction.

-   A smaller topic (Topic 0) highlights issues with cleanliness and family experience, which while less common, may point to isolated but impactful problems ‚Äî especially for visitors with children.

Contextual Insight

This analysis supports earlier findings from the word cloud and sentiment breakdown:

-   Tourists, who make up the vast majority of reviewers for Disneyland Paris, are particularly sensitive to time management, value for money, and service quality.

-   The lack of local reviewers means there‚Äôs less balancing feedback from visitors with lower expectations or more familiarity with the park experience.

Together, these topics provide actionable insight into why Disneyland Paris has a higher share of negative reviews, helping guide potential areas for improvement such as queue management, service training, and cleanliness.

### Frequency of Topics in Negative Disneyland Paris Reviews

Using **BERTopic**, we extracted and categorized common themes in 1,000 negative reviews of **Disneyland Paris**. The chart above shows the frequency of each topic, based on semantic clustering of similar reviews.

```{r}

topic_info <- topic_model$get_topic_info()
topic_info_df <- py_to_r(topic_info)




topic_info_df %>%
  filter(Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in Negative Disneyland Paris Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()


```

#### Top 5 Negative Themes Identified:

1.  Time, Staff, Fast Food\
    ‚Äî The dominant topic, with nearly 400 mentions\
    ‚Äî Complaints about long waits, staff interactions, and poor food quality

2.  Food, Expensive, Queues, Overpriced\
    ‚Äî Highlights issues with high prices, long queues, and perceived lack of value

3.  Closed Rides (Pirates, Mountain, Caribbean)\
    ‚Äî Visitors frustrated with major rides being closed or unavailable

4.  Smoking, Smoke, People, Children\
    ‚Äî Concerns about second-hand smoke exposure, particularly around children

5.  Toilets, Dirty, Staff, Disgusting\
    ‚Äî Hygiene and cleanliness issues, especially with bathrooms and service areas

### Key Takeaways

-   The most dominant issues revolve around time management, food service, and staff experience ‚Äî these align strongly with earlier findings from the word cloud and sentiment classification.

-   Many complaints are operational and solvable, such as managing ride closures, cleanliness, and crowd flow.

-   The recurring mention of staff across several topics suggests a need for staff training or service consistency, especially for a tourist-heavy audience.

This topic breakdown helps explain why Disneyland Paris receives significantly more negative reviews than Disneyland California, especially from tourists:

Tourists are less forgiving of inefficiencies and tend to have higher expectations ‚Äî particularly when traveling internationally. With Paris relying almost entirely on tourist reviewers, these operational issues are amplified in the review data.

This insight can guide targeted service improvements to raise satisfaction and reputation among future visitors.

### Visualizing Topic Clusters with UMAP

To better understand the distribution of themes in negative reviews at Disneyland Paris, we applied UMAP (Uniform Manifold Approximation and Projection) to reduce the high-dimensional text embeddings into two dimensions. Each point represents a single review, and colors indicate the topic assigned by the BERTopic model.

```{r}

embeddings <- as.data.frame(py_to_r(topic_model$umap_model$embedding_)) 

colnames(embeddings) <- c("UMAP1", "UMAP2")
topics <- reticulate::py_to_r(topic_results[[1]])
embeddings$topics <- as.factor(topics)
# Remove NA-named columns
embeddings <- embeddings[, !is.na(names(embeddings)) & names(embeddings) != ""]

# Now safe to filter
embeddings <- embeddings %>% filter(topics != -1)


# trimming of outliers
ggplotly(
  ggplot(embeddings) +
    geom_point(aes(UMAP1, UMAP2, color = topics)) +
    theme_classic()
)


```

#### Key Observations:

-   The plot reveals two main clusters:

    -   Topic 1 (blue): The dominant group, covering the majority of negative reviews. These reviews focus on issues like queues, staff, food quality, and long waiting times.

    -   Topic 0 (red): A smaller, well-separated cluster with more specific issues like hygiene, toilets, and children-related complaints.

-   The distinct spatial separation of Topic 0 suggests that this cluster is semantically unique ‚Äî i.e., reviews in this group are notably different in tone and content from the rest.

-   The density and volume of Topic 1 reflect its role as the primary driver of dissatisfaction, consistent with earlier frequency charts and word clouds.

Why This Visualization Matters

This clustering provides strong qualitative support for the quantitative findings:

-   Most negative sentiment is concentrated around operational frustrations (time, service, food).

-   A smaller but cohesive subgroup of complaints relates to cleanliness and child-friendliness, which could represent high-impact issues for families.

By visualizing the semantic landscape of negative feedback, we gain an intuitive understanding of how complaints are grouped ‚Äî helping Disneyland Paris prioritize both broad systemic improvements and niche but critical pain points.

### Topic Modeling of Positive Reviews ‚Äì Disneyland California

To understand why Disneyland California receives the most positive reviews, we applied BERTopic to cluster and summarize key themes found in positive sentiment reviews.

Using the same multilingual embedding model, the reviews were grouped into semantically meaningful topics. Only unique (non-duplicate) reviews were included, and each review was truncated for processing efficiency.

```{r}


# Filter out text that appears more than once
emo_results <- emo_results %>%
  filter(duplicate_count == 1)

reviews_california_pos <- emo_results %>%
  filter(Branch == "Disneyland_California", label == "POSITIVE",  !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean)

reviews_california_pos <- reviews_california_pos %>%
  str_squish() %>%
  str_trunc(300)  # if input is long

# Use same embedding model as before
topic_model_california <- bert$BERTopic(
  embedding_model = embedding_model,
  min_topic_size = 30L,
  calculate_probabilities = TRUE,
  verbose = TRUE
)

topic_results_california <-topic_model_california$fit_transform(reviews_california_pos)
topic_model_california$get_topic_info()

topic_info_2 <- topic_model_california$get_topic_info()
topic_info_df_2 <- py_to_r(topic_info_2)




topic_info_df_2 %>%
  filter(Count >20, Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in positive California Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()




```

Why This Matters

This breakdown confirms that Disneyland California‚Äôs positive reviews are rich and diverse, with a strong focus on:

-   Efficient logistics (fast passes, wait times)

-   Emotional connection and nostalgia

-   Family experiences and kid-friendly environments

-   Special events like Halloween and Christmas

-   Entertainment value (fireworks, parades)

-   Cleanliness and staff friendliness

These themes are notably absent or negatively framed in reviews of Disneyland Paris ‚Äî especially regarding staff and time efficiency ‚Äî reinforcing your earlier findings.

Final Insight

The high volume of local visitors in California may explain the more positive, familiar, and emotionally rich feedback ‚Äî especially compared to Disneyland Paris, which sees more critical feedback from tourists expecting a flawless once-in-a-lifetime experience.

## Part 3: Locations analysis

üî∑ What differences can we detect for the three different locations and are there any interesting trends over time?

To explore how visitor feedback varies over time, we analyzed monthly review counts for each Disneyland location from 2010 to 2019. The line chart above shows review volume by month, year, and branch (California, Hong Kong, and Paris).

```{r}
reviews <- reviews %>%
  mutate(
    Year_Month = as_date(Year_Month),
    month = month(Year_Month, label = TRUE, abbr = TRUE),
    year = year(Year_Month)
  )

monthly_reviews <- reviews %>%
  filter(!is.na(month) & !is.na(year)) %>%
  count(year, month, Branch)

# Sort month correctly as a factor
monthly_reviews$month <- factor(monthly_reviews$month, 
                                 levels = month.abb, 
                                 ordered = TRUE)

# Line plot
ggplot(monthly_reviews, aes(x = month, y = n, color = as.factor(year), group = year)) +
  geom_line() +
  facet_wrap(~ Branch, scales = "free_y") +
  labs(title = "Monthly Review Trends per Year and Branch",
       x = "Month", y = "Number of Reviews", color = "Year") +
  theme_minimal()

```

Key Observations:

Seasonality across all parks:

-   All three parks show consistent peaks in review activity from May to August, aligning with summer holidays in many countries.

-   There's often a secondary rise in December, likely due to holiday events and winter travel.

Disneyland California:

-   The most active park in terms of reviews overall.

-   Clear seasonal peaks occur around June to August, especially in 2014‚Äì2016.

-   An unusually strong post-summer rebound in December is visible in several years, likely driven by Thanksgiving and Christmas traffic.

-   The review volume dropped somewhat after 2017, which may reflect platform or behavior changes rather than actual visitation.

Disneyland Hong Kong:

-   Reviews peaked in spring and early summer (April‚ÄìJuly), with fewer December spikes compared to other locations.

-   Fluctuations are more moderate, and volumes are lower than California, but follow a similar summer-dominated pattern.

-   Notable review growth in 2014 and 2015, then slight decline afterward.

Disneyland Paris:

-   Seasonal trends are very pronounced, with major spikes in July and August each year.

-   This suggests a strong reliance on European summer tourism, particularly school holiday travelers.

-   Activity decreases sharply in September and remains low through the fall, with less December review activity compared to California.

Why These Peaks Happen

-   Summer (June‚ÄìAugust): School vacations, family travel, and warm weather drive the bulk of visitation.

-   December: Holiday events (e.g., Christmas parades, themed d√©cor) often lead to increased reviews, particularly in California.

-   April and October spikes (in some years): Often coincide with spring breaks and Halloween events, especially in California and Hong Kong.

Conclusion

This analysis shows that while all parks follow seasonal review trends, they vary in intensity and consistency:

-   California leads in volume and exhibits both summer and winter peaks.

-   Paris is highly summer-dependent, with less consistent winter activity.

-   Hong Kong has more muted fluctuations, possibly due to regional travel behavior or review habits.

This temporal context is essential when interpreting sentiment and content ‚Äî a surge in reviews may coincide with both excitement and operational stress, impacting the tone and topics of the reviews.

### Average Rating Trends Over Time by Location

The line plot shows the average visitor rating (1‚Äì5) for each Disneyland location from 2010 to 2019. This highlights how guest satisfaction has changed over time and varies between the three parks.

```{r}
reviews %>%
  group_by(Year, Branch) %>%
  summarise(avg_rating = mean(Rating, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = avg_rating, color = Branch)) +
  geom_line(size = 1.2) +
  labs(title = "Average Rating Over the Years by Location",
       x = "Year",
       y = "Average Rating") +
  theme_minimal()


```

Key Insights:

Disneyland California (Red):

-   Maintains the highest average rating overall across the decade.

-   While the average rating shows a slight decline after 2015, it remains relatively stable and high, suggesting consistent visitor satisfaction.

-   The drop around 2016‚Äì2018 may reflect increased expectations or operational challenges, but sentiment rebounded in 2019.

Disneyland Hong Kong (Green):

-   Shows a sharp increase in ratings from 2010 to 2013, stabilizing around 4.2‚Äì4.3.

-   Slight decline begins post-2017, possibly due to crowding, maintenance, or local events.

-   Despite this, Hong Kong holds a mid-position, generally outperforming Paris but not quite reaching California‚Äôs consistency.

Disneyland Paris (Blue):

-   Shows lower average ratings throughout the period.

-   Brief improvement between 2012 and 2016, peaking just above 4.1.

-   From 2016 onward, ratings consistently decline, dropping below 3.8 by 2019 ‚Äî a clear sign of growing visitor dissatisfaction.

Interpretation in Context

This long-term trend helps explain earlier findings:

-   California's higher local visitor ratio likely contributes to steadier, more forgiving feedback.

-   Paris relies heavily on tourists, whose higher expectations and more sensitive experiences may amplify negative feedback over time.

-   External factors like renovations, staff shortages, or seasonal overcrowding could explain sudden drops (especially Paris in 2017‚Äì2019).

This time-based analysis confirms that Disneyland California not only receives more positive reviews overall, but sustains high satisfaction consistently over the years. Meanwhile, Disneyland Paris faces growing dissatisfaction, making it a clear candidate for operational review and experience improvement.

### Sentiment Trend Over the Years by Branch

This chart tracks the percentage of reviews classified as positive, neutral, or negative each year from 2010 to 2019 for Disneyland California, Hong Kong, and Paris. It provides a clear look at how public perception of each park has changed over time.

```{r}
sentiment_by_year <- emo_results %>%
  group_by(Branch, Year, label) %>%
  summarise(count = n(), .groups = "drop")

sentiment_normalized_yearly <- sentiment_by_year %>%
  group_by(Branch, Year) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()

ggplot(sentiment_normalized_yearly, aes(x = Year, y = percent, color = label)) +
  geom_line(size = 1.2) +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment Trend Over the Years by Branch",
    x = "Year",
    y = "Percentage of Reviews",
    color = "Sentiment"
  ) +
  theme_minimal()
```

Disneyland California:

-   Maintains a consistently high proportion of positive sentiment, averaging above 60%.

-   Negative sentiment remains relatively low, with only slight increases in 2017‚Äì2018 (coinciding with minor dips in star ratings).

-   Neutral reviews remain low and stable.

Interpretation: California‚Äôs strong local visitor base and consistent service likely help maintain positive emotional responses, even when operational issues arise.

Disneyland Hong Kong:

-   Shows a significant shift toward positive sentiment after 2010.

-   From 2011 onward, positive reviews dominate, making up \~60‚Äì65% annually.

-   Negative sentiment sharply declined after 2010 and remained low, suggesting successful improvements early in the decade.

Interpretation: The early 2010s may have seen impactful quality upgrades or marketing improvements. Sentiment remains fairly strong, though not quite as consistently high as California.

Disneyland Paris:

-   Displays the most dramatic shift in sentiment:

    -   In 2010‚Äì2011, positive sentiment dominated.

    -   From 2012 onward, there‚Äôs a steady decline in positivity, dropping below 50% by 2018‚Äì2019.

    -   Meanwhile, negative sentiment rises, matching or exceeding positive reviews in recent years.

-   Neutral sentiment remains relatively flat.

Interpretation: Paris is experiencing a clear sentiment erosion. This aligns with prior findings of:

-   A large share of tourist reviewers

-   Complaints around queues, food, staff, cleanliness

-   Low average ratings in recent years

Summary

This sentiment timeline reinforces the key finding:

Disneyland California earns and sustains the best emotional response over time, while Disneyland Paris is experiencing growing dissatisfaction ‚Äî especially from tourists.

These trends are valuable for long-term operational monitoring and can help prioritize improvements for guest experience, crowd control, and service training, particularly in Paris.

### Visitor Composition by Disneyland Branch

This chart shows the distribution of local vs. tourist reviewers across the three Disneyland locations, based on matching each reviewer‚Äôs stated location with the park's country.

```{r}
# Branch-country mapping
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join and classify visitor type
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

# Summarize and reshape data
summary_data <- reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(
    Total = Tourist + Local,
    Tourist_Ratio = Tourist / Total
  )

# Reshape for plotting
plot_data <- summary_data %>%
  pivot_longer(cols = c("Local", "Tourist"), names_to = "Visitor_Type", values_to = "Count")

# Plot
ggplot(plot_data, aes(x = Branch, y = Count, fill = Visitor_Type)) +
  geom_bar(stat = "identity") +
  geom_text(
    data = summary_data,
    aes(x = Branch, y = Total + 10, label = scales::percent(Tourist_Ratio, accuracy = 1)),
    inherit.aes = FALSE
  ) +
  labs(title = "Visitor Composition by Disneyland Branch",
       y = "Number of Visitors",
       x = "Branch") +
  theme_minimal() +
  scale_fill_manual(values = c("Local" = "yellow", "Tourist" = "#FF7415")) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

```

Key Findings:

-   Disneyland California has the most balanced audience:

    -   \~36% local visitors ‚Äî significantly more than other parks.

    -   \~64% of reviews come from tourists.

-   Disneyland Hong Kong:

```         
-    \~95% tourists, just 5% local reviews.

-   Still, Hong Kong performs relatively well in ratings and sentiment.
```

-   Disneyland Paris:

```         
-    Overwhelmingly tourist-driven, with 99% of reviews from international visitors.

-   This aligns with its consistently lower sentiment and rating performance.
```

Why This Matters

Local visitors are often:

-   More familiar with the park experience

-   Less critical of occasional shortcomings

-   More likely to visit repeatedly, reducing the pressure for a ‚Äúperfect‚Äù experience

Tourists, on the other hand:

-   May face language or cultural barriers

-   Are often first-time visitors with high expectations

-   Experience the park as a one-time destination, meaning any frustration can lead to more negative reviews

This helps explain the higher sentiment and ratings for Disneyland California and the growing dissatisfaction seen at Disneyland Paris.

Final Insight

The visitor composition is a powerful predictor of review tone. Parks with more local engagement (like California) tend to receive more positive, forgiving, and emotionally rich reviews. Parks relying heavily on tourists (like Paris) face more scrutiny and dissatisfaction ‚Äî especially when logistical or service-related issues arise.

This makes visitor type a critical consideration for interpreting sentiment, planning service improvements, and targeting future marketing strategies.

## Part 4: Conclusion and Discussion

üî∑ What specific advice can we give to the Park management based on our analysis? How can we integrate the analysis of reviews in internal processes, can we think of any data products that would be of value?

Over the course of this analysis, we explored review data from Disneyland California, Paris, and Hong Kong to understand how visitor experiences differ by location, time, and reviewer type. By examining ratings, sentiment trends, thematic topics, and the composition of reviewers, we uncovered actionable insights that can support more informed decision-making and experience design across the parks.

One of the most consistent findings is that Disneyland California performs best in terms of sentiment and average ratings. This park not only receives the most reviews but also sustains a high level of visitor satisfaction over time. A key contributing factor appears to be its balanced visitor base ‚Äî around 36% of reviews come from locals. These local visitors tend to be more familiar with the park, more forgiving of minor inconveniences, and often more emotionally attached to the Disneyland brand. Their reviews are more positive on average and contribute to a generally stable reputation.

In contrast, Disneyland Paris receives the lowest ratings and most negative sentiment, with a clear downward trend over the last decade. This park is reviewed almost exclusively by tourists (99%), many of whom are first-time visitors with high expectations. When these expectations are not met ‚Äî due to long queues, unclean facilities, or staff-related frustrations ‚Äî tourists are more likely to leave critical reviews. Topic modeling of negative reviews from Paris revealed common issues such as long wait times, unclean toilets, staff rudeness, lack of shows, and a perceived lack of value for money.

From this analysis, it's evident that Disneyland Paris has a significant opportunity to improve the guest experience by directly addressing the topics most associated with negative feedback. Management could consider expanding queue management systems, such as virtual lines or FastPass-like features, to alleviate frustration related to long waits. Moreover, enhancing the entertainment offering ‚Äî including more frequent fireworks, parades, and seasonal shows ‚Äî could create the type of magical experiences that guests consistently praise in California.

In addition to operational improvements, Paris would benefit from investing in staff training programs that emphasize friendliness, multilingual support, and responsiveness. Many tourists may already be navigating cultural and logistical challenges; a warm, welcoming atmosphere can greatly shape their overall perception. Furthermore, the frequent appearance of complaints about unclean toilets and public areas suggests that improving cleanliness standards is not just a hygiene issue, but also a reputational one.

Beyond immediate actions, there are ways to integrate review analysis into ongoing park management. For example, sentiment trends could be monitored through internal dashboards, providing real-time feedback on visitor satisfaction. Parks could also adopt automated alerts that flag emerging negative topics before they become widespread issues. Review data could even be used in staff performance feedback, training design, or in prioritizing maintenance and refurbishment schedules.

There is also potential to develop data products that support operational decisions. A ‚Äúvisitor sentiment score,‚Äù updated weekly by location or attraction, could become a standard metric used in executive reports. Topic heatmaps showing where certain types of complaints cluster spatially within a park could help inform layout changes or staff deployment. These kinds of tools would make sentiment analysis not just a post-hoc reflection tool but an integral part of the guest experience feedback loop.

In conclusion, visitor reviews are an incredibly rich and underutilized source of insight. They capture authentic guest perspectives at scale and can guide both strategic and tactical improvements. Disneyland California shows what consistent, positive experiences can yield, while Disneyland Paris shows where missed expectations can erode public sentiment over time. By responding directly to review data ‚Äî especially in Paris ‚Äî Disney can protect its brand reputation and continue delivering magical experiences that live up to the promise of its parks.
