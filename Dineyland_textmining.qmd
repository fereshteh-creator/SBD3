---
title: "Text_Mining"
author: "Fereshteh Ahmadi, Seraina Felicitas Zimmermann, Michael Etter, Donjet Dzemaili"
format:
  html:
    df-print: paged
    toc: true
    code-tools: true

editor: visual
---

# Review Analysis for Disneyland

## Part 0: Prepossessing

### Environment and Setting

Please set your own virtual environment if needed

```{r}
#| label: choose_environment


reticulate::use_condaenv("huggingfaceR", required = TRUE)
set.seed(19)

```

### Libraries

```{r}
#| label: load_packages
#| message: false
#| warning: false
#| code-fold: true

library(tidyverse)            # Core data wrangling and visualization packages (dplyr, ggplot2, etc.)
library(quanteda)             # Text tokenization, preprocessing, and creating document-feature matrices
library(stopwords)            # Provides multilingual stopword lists for text cleaning
library(topicmodels)          # Topic modeling algorithms like LDA and CTM (Latent Dirichlet Allocation)
library(tidytext)             # Tidy-friendly tools for text mining and sentiment analysis
library(quanteda.textplots)   # Visualization tools for quanteda objects (word clouds, networks, etc.)
library(ggplot2)              # Grammar of graphics plotting system (used by tidyverse)
library(scales)               # Formatting and scaling functions for plots (e.g., percentages, dates)
library(devtools)             # Tools to install and manage R packages from GitHub
library(plotly)               # Interactive plots (works well with ggplot2 for interactivity)
library(readr)                # Efficient functions to read data (like CSVs) â€” part of tidyverse
library(ldatuning)            # Helps choose optimal number of topics in LDA models
library(bertopic)             # (If installed) R wrapper or interface to the Python BERTopic library
library(htmltools)            # Used in HTML report rendering and interactive outputs (like word clouds)
library(umap)                 # Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction
library(reticulate)

if (!require("cld2")) install.packages("cld2")
if (!require("hunspell")) install.packages("hunspell")
library(cld2) # for language detection
library(hunspell) # for spelling correction

options(scipen=999)



```

### Data

This dataset contains user-generated reviews for Disneyland parks in Hong Kong, California, and Paris. It captures various attributes of each review, enabling analysis of visitor feedback across locations and over time. The dataset includes the following columns:

-   review_id: A unique identifier assigned to each review.

-   Rating: A numerical score provided by the reviewer, indicating their overall satisfaction.

-   Year_Month: The year and month when the review was posted.

-   Reviewer Location: The geographical location of the reviewer, as stated in their profile.

-   Year: The year when the review was written, extracted from the Year_Month field.

-   Review Text: The full text of the review, expressing the reviewer's experience and opinions.

-   Language: The language in which the review was written.

```{r}
load('Disneyland.rda')
head(reviews)
```

### Data tidying

To prepare the Disneyland review data for text analysis, a series of text preprocessing steps were performed to clean and standardize the review content. The goal of this process is to remove irrelevant information, reduce noise, and create a clean corpus suitable for natural language processing (NLP) tasks such as sentiment analysis, keyword extraction, or topic modeling.

1.  Stopword Loading: Common English stopwords were imported using the `tidytext` package.

2.  Text Cleaning: All reviews were converted to lowercase, and unwanted elements like URLs, HTML tags, punctuation, numbers, and extra spaces were removed. Reviews with missing text were excluded.

3.  Custom Stopwords: A custom list of frequently repeated and non-informative wordsâ€”such as "disney", "ride", "park", and location namesâ€”was added to the stopword list.

4.  Tokenization and Filtering: Reviews were split into individual words (tokens), and all standard and custom stopwords were removed.

5.  Reconstruction: The remaining tokens were reassembled into cleaned review texts for each review ID.

6.  Merging: The cleaned text was merged back with the original dataset, preserving all metadata.

This process ensures that only the most meaningful and relevant words remain, improving the quality and accuracy of downstream text analysis.

```{r}

# Load stopwords
data("stop_words")

# remove symbols 
reviews_clean <- reviews %>%
  filter(!is.na(Review_Text)) %>%
  mutate(Review_ID,
         Review_Text = Review_Text %>%
           str_to_lower() %>%
           str_replace_all("http\\S+|www\\S+", "") %>%
           str_replace_all("<.*?>", "") %>%
           str_replace_all("[^a-z\\s]", " ") %>%
           str_squish()) 

custom_stops <- tibble(
  word = c("disney", "disneyland", "paris", "hong", "kong", "california", "ride", "rides", "park", "parks", "day")
)
all_stopwords <- bind_rows(stop_words, custom_stops)

# Tokenize and remove stopwords
tokens <- reviews_clean %>%
  unnest_tokens(word, Review_Text) %>%
  anti_join(all_stopwords, by = "word")

# Rebuild cleaned text from tokens and join back to full data
reconstructed <- tokens %>%
  group_by(Review_ID) %>%
  summarise(Review_Text_clean = str_c(word, collapse = " "), .groups = "drop")

# Join back to original metadata
reviews_clean <- reviews_clean %>%
  left_join(reconstructed, by = "Review_ID")

```

-   **Multilingual Reviews:**

Before performing text analysis, we first checked whether the reviews were written in multiple languages. Using a language detection function, the language of each review was identified based on its text content:

-   The `detect_language()` function was applied to the `Review_Text` column to assign a detected language code (e.g., "en" for English) to each review.

-   A filter was then applied to identify any reviews not written in English.

This step ensures that only English-language reviews are included in the analysis, avoiding noise or misinterpretation caused by multilingual content. Since its difficult to fully recognize other languages we will use multilingual models.

```{r}

# Detect language of each review
reviews$language <- detect_language(reviews$Review_Text)

# See if there are other languages in the data
reviews[reviews$language!= "en", ]


```

-   **Fake or Sponsored Reviews:**

To improve data quality and reduce the influence of spam or low-effort content, we implemented checks to identify and remove potentially fake or sponsored reviews. The filtering process involved two key steps:

1.   Short Review Removal:\
    Very short reviews (fewer than 10 words) were identified and excluded. These were predominantly 5-star ratings with minimal content, often lacking meaningful information for analysis.

2.  Duplicate Review Removal:\
    Reviews with identical cleaned text (`Review_Text_clean`) were flagged as duplicates, likely indicating copy-pasted or spammed content. Only unique reviews were retained by removing entries that appeared more than once.

This step helps ensure that the final dataset is composed of authentic, original, and content-rich reviews, enhancing the reliability of subsequent text analyses.

```{r}
# very short reviews seem to be mostly 5 star ratings
reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  arrange(word_count) %>%
  select(Rating, Review_Text, word_count) %>% 
  filter(word_count<10)

reviews_clean<-reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  filter(word_count>10) %>% 
  group_by(Review_Text_clean) %>% 
    mutate(duplicate_count = n()) %>%
    ungroup() %>% 
    filter(duplicate_count == 1)


    
```

-   **Spelling Mistakes:**\

To solve this problem we mainly use NLPs.

--\> sie hat was darÃ¼ber gesagt aber weisss es nicht mehr. MÃ¼ssen wir das noch machen?

## part 1: Content analysis and topic modeling

ðŸ”· What can we tell about the customers that write reviews?

```{r}
# most of the customers write shorter reviews
reviews %>%
  mutate(length = nchar(Review_Text)) %>%
  ggplot(aes(x = length)) +
  geom_histogram(binwidth = 20)

```

Most of the reviews are for California and mostly from 2015-2017

```{r}

# Extract year
reviews$year <- year(reviews$Year)

# Plot: Reviews by year and branch
reviews %>%
  count(year, Branch) %>%
  ggplot(aes(x = year, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Year and Branch",
       x = "Year", y = "Number of Reviews") +
  theme_minimal()

```

seems like peek visits in California are in July, in Paris August and in HongKong December

```{r}
# Extract month (abbreviated names for labels)
reviews$month <- month(reviews$Year_Month, label = TRUE, abbr = TRUE)

# Plot: Reviews by month and branch
reviews %>%
  count(month, Branch) %>%
  ggplot(aes(x = month, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Month and Branch",
       x = "Month", y = "Number of Reviews") +
  theme_minimal()

```

Seems like california has the lowest ratio of tourists who left a comment.

```{r}
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join to get park country
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  tidyr::pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(Total = Tourist + Local,
         Tourist_Ratio = Tourist / Total)

```

## Part 2: Sentiment analysis

ðŸ”· What do the visitors talk about in their reviews and how does it relate sentiment/ratings?

```{r}
set.seed(9) 

# Shuffle and sample the data
sampled_reviews <- reviews_tagged %>%
  filter(!is.na(Review_Text_clean)) %>%
  slice_sample(n = 10000) %>% 
  mutate(Review_Text_clean = str_trunc(Review_Text_clean, 450))  # about 512 tokens after tokenization

# Load Python's transformers package
transformers <- import("transformers")

# Load sentiment analysis pipeline
sentiment_model <- transformers$pipeline(
  task = "text-classification",
  model = "nlptown/bert-base-multilingual-uncased-sentiment"
)


# Enable tokenizer parallelism 
Sys.setenv(TOKENIZERS_PARALLELISM = "true")

emo_results <- sampled_reviews %>%
  mutate(
    sentiment = map(Review_Text_clean, safely(sentiment_model))
  ) %>%
  mutate(
    sentiment_result = map(sentiment, ~ .x$result[[1]]),
    stars = map_dbl(sentiment_result, ~ as.numeric(str_extract(.x$label, "\\d"))),
    label = case_when(
      stars <= 2 ~ "NEGATIVE",
      stars == 3 ~ "NEUTRAL",
      stars >= 4 ~ "POSITIVE"
    )
  )

# Disable parallelism again     
Sys.setenv(TOKENIZERS_PARALLELISM = "false")


 
```

ratings dont seem so good for paris

```{r}

emo_results %>%
  group_by(Branch) %>%
  summarise(avg_sentiment = mean(Rating, na.rm = TRUE))

```

Paris has many negative reviews why?

```{r}

sentiment_by_visitor <- emo_results %>%
  group_by(Branch, Visitor_Type, label) %>%
  summarise(count = n(), .groups = "drop")


sentiment_normalized <- sentiment_by_visitor %>%
  group_by(Branch, Visitor_Type) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()


ggplot(sentiment_normalized, aes(x = Visitor_Type, y = percent, fill = label)) +
  geom_col(position = "dodge") +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment (%) by Visitor Type and Branch",
    x = "Visitor Type",
    y = "Percentage of Reviews",
    fill = "Sentiment"
  ) +
  theme_minimal()



```

Look at words or maybe cloud of words

```{r}

#| label: wordcloud

custom_stopwords <- c("disneyland", "disney", "park", "day")

# disneyland Paris
paris_tokens <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label=="NEGATIVE", !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean) %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(pattern = custom_stopwords)


paris_dfm <- dfm(paris_tokens)
paris_dfm <- paris_dfm[ntoken(paris_dfm) > 0, ]

textplot_wordcloud(
  paris_dfm,
  min_count = 10,
  max_words = 100,
  color = "darkblue",
  min_size = 0.5,
  max_size = 4
)


```

Lets use bertopic to find out what they are talking about

```{r}
# Load Python modules
bert <- import("bertopic")
sentence_transformers <- import("sentence_transformers")

# Embedding model (multilingual)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

topic_model <- bert$BERTopic(
  embedding_model = embedding_model,
  nr_topics = r_to_py(6L),
  calculate_probabilities = TRUE,
  verbose = TRUE
)

docs <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label == "NEGATIVE", !is.na(Review_Text_clean)) %>%
  slice_sample(n = 1000) %>%
  pull(Review_Text_clean)

# Fit model (run this in console, not inside Rmd chunk) it works for me though
topic_results <- topic_model$fit_transform(docs)

# Get summary of topics
topic_model$get_topic_info()






   
```

Lets visulize

```{r}

topic_info <- topic_model$get_topic_info()
topic_info_df <- py_to_r(topic_info)




topic_info_df %>%
  filter(Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in Negative Disneyland Paris Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()


```

```{r}

embeddings <- as.data.frame(py_to_r(topic_model$umap_model$embedding_)) 

colnames(embeddings) <- c("UMAP1", "UMAP2")
topics <- reticulate::py_to_r(topic_results[[1]])
embeddings$topics <- as.factor(topics)
# Remove NA-named columns
embeddings <- embeddings[, !is.na(names(embeddings)) & names(embeddings) != ""]

# Now safe to filter
embeddings <- embeddings %>% filter(topics != -1)


# trimming of outliers
ggplotly(
  ggplot(embeddings) +
    geom_point(aes(UMAP1, UMAP2, color = topics)) +
    theme_classic()
)


```

now lets do the same thing for California positive since they have the most positive reviews in our sample.

```{r}


# Filter out text that appears more than once
emo_results <- emo_results %>%
  filter(duplicate_count == 1)

reviews_california_pos <- emo_results %>%
  filter(Branch == "Disneyland_California", label == "POSITIVE",  !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean)

reviews_california_pos <- reviews_california_pos %>%
  str_squish() %>%
  str_trunc(300)  # if input is long

# Use same embedding model as before
topic_model_california <- bert$BERTopic(
  embedding_model = embedding_model,
  min_topic_size = 30L,
  calculate_probabilities = TRUE,
  verbose = TRUE
)

topic_results_california <-topic_model_california$fit_transform(reviews_california_pos)
topic_model_california$get_topic_info()

topic_info_2 <- topic_model_california$get_topic_info()
topic_info_df_2 <- py_to_r(topic_info_2)




topic_info_df_2 %>%
  filter(Count >20, Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in positive California Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()




```

based on these insights, paris could work on negative topics and offer more fastpass, shows and events like firework, special attractions, friendlier staff and cleaner toilets.

## Part 3: Locations analysis

ðŸ”· What differences can we detect for the three different locations and are there any interesting trends over time?

```{r}
reviews <- reviews %>%
  mutate(
    Year_Month = as_date(Year_Month),
    month = month(Year_Month, label = TRUE, abbr = TRUE),
    year = year(Year_Month)
  )

monthly_reviews <- reviews %>%
  filter(!is.na(month) & !is.na(year)) %>%
  count(year, month, Branch)

# Sort month correctly as a factor
monthly_reviews$month <- factor(monthly_reviews$month, 
                                 levels = month.abb, 
                                 ordered = TRUE)

# Line plot
ggplot(monthly_reviews, aes(x = month, y = n, color = as.factor(year), group = year)) +
  geom_line() +
  facet_wrap(~ Branch, scales = "free_y") +
  labs(title = "Monthly Review Trends per Year and Branch",
       x = "Month", y = "Number of Reviews", color = "Year") +
  theme_minimal()

```

```{r}
# Durchschnittliches Rating pro Jahr (und ggf. pro Standort)
reviews %>%
  group_by(Year, Branch) %>%
  summarise(avg_rating = mean(Rating, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = avg_rating, color = Branch)) +
  geom_line(size = 1.2) +
  labs(title = "Average Rating Over the Years by Location",
       x = "Year",
       y = "Average Rating") +
  theme_minimal()


```

Sentimental

```{r}
sentiment_by_year <- emo_results %>%
  group_by(Branch, Year, label) %>%
  summarise(count = n(), .groups = "drop")

sentiment_normalized_yearly <- sentiment_by_year %>%
  group_by(Branch, Year) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()

ggplot(sentiment_normalized_yearly, aes(x = Year, y = percent, color = label)) +
  geom_line(size = 1.2) +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment Trend Over the Years by Branch",
    x = "Year",
    y = "Percentage of Reviews",
    color = "Sentiment"
  ) +
  theme_minimal()
```

Visitor or Tourist:

```{r}
# Branch-country mapping
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join and classify visitor type
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

# Summarize and reshape data
summary_data <- reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(
    Total = Tourist + Local,
    Tourist_Ratio = Tourist / Total
  )

# Reshape for plotting
plot_data <- summary_data %>%
  pivot_longer(cols = c("Local", "Tourist"), names_to = "Visitor_Type", values_to = "Count")

# Plot
ggplot(plot_data, aes(x = Branch, y = Count, fill = Visitor_Type)) +
  geom_bar(stat = "identity") +
  geom_text(
    data = summary_data,
    aes(x = Branch, y = Total + 10, label = scales::percent(Tourist_Ratio, accuracy = 1)),
    inherit.aes = FALSE
  ) +
  labs(title = "Visitor Composition by Disneyland Branch",
       y = "Number of Visitors",
       x = "Branch") +
  theme_minimal() +
  scale_fill_manual(values = c("Local" = "yellow", "Tourist" = "#FF7415")) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

```

## Part 4: Conclusion and Discussion

ðŸ”· What specific advice can we give to the Park management based on our analysis? How can we integrate the analysis of reviews in internal processes, can we think of any data products that would be of value?
