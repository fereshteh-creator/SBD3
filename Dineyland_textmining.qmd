---
title: "Text_Mining"
author: "Fereshteh Ahmadi, Seraina Felicitas Zimmermann, Michael Etter, Donjet Dzemaili"
format:
  html:
    df-print: paged
    toc: true
    code-tools: true

editor: visual
---

# Review Analysis for Disneyland

## Part 0: Prepossessing

### Environment and Setting

Please set your own virtual environment if needed

```{r}
#| label: choose_environment


reticulate::use_condaenv("huggingfaceR", required = TRUE)
set.seed(19)

```

### Libraries

```{r}
#| label: load_packages
#| message: false
#| warning: false
#| code-fold: true

library(tidyverse)            # Core data wrangling and visualization packages (dplyr, ggplot2, etc.)
library(quanteda)             # Text tokenization, preprocessing, and creating document-feature matrices
library(stopwords)            # Provides multilingual stopword lists for text cleaning
library(topicmodels)          # Topic modeling algorithms like LDA and CTM (Latent Dirichlet Allocation)
library(tidytext)             # Tidy-friendly tools for text mining and sentiment analysis
library(quanteda.textplots)   # Visualization tools for quanteda objects (word clouds, networks, etc.)
library(ggplot2)              # Grammar of graphics plotting system (used by tidyverse)
library(scales)               # Formatting and scaling functions for plots (e.g., percentages, dates)
library(devtools)             # Tools to install and manage R packages from GitHub
library(plotly)               # Interactive plots (works well with ggplot2 for interactivity)
library(readr)                # Efficient functions to read data (like CSVs) ‚Äî part of tidyverse
library(ldatuning)            # Helps choose optimal number of topics in LDA models
library(bertopic)             # (If installed) R wrapper or interface to the Python BERTopic library
library(htmltools)            # Used in HTML report rendering and interactive outputs (like word clouds)
library(umap)                 # Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction
library(reticulate)

if (!require("cld2")) install.packages("cld2")
if (!require("hunspell")) install.packages("hunspell")
library(cld2) # for language detection
library(hunspell) # for spelling correction

options(scipen=999)



```

### Data

This dataset contains user-generated reviews for Disneyland parks in Hong Kong, California, and Paris. It captures various attributes of each review, enabling analysis of visitor feedback across locations and over time. The dataset includes the following columns:

-   review_id: A unique identifier assigned to each review.

-   Rating: A numerical score provided by the reviewer, indicating their overall satisfaction.

-   Year_Month: The year and month when the review was posted.

-   Reviewer Location: The geographical location of the reviewer, as stated in their profile.

-   Year: The year when the review was written, extracted from the Year_Month field.

-   Review Text: The full text of the review, expressing the reviewer's experience and opinions.

-   Language: The language in which the review was written.

```{r}
load('Disneyland.rda')
head(reviews)
```

### Data tidying

To prepare the Disneyland review data for text analysis, a series of text preprocessing steps were performed to clean and standardize the review content. The goal of this process is to remove irrelevant information, reduce noise, and create a clean corpus suitable for natural language processing (NLP) tasks such as sentiment analysis, keyword extraction, or topic modeling.

1.  Stopword Loading: Common English stopwords were imported using the `tidytext` package.

2.  Text Cleaning: All reviews were converted to lowercase, and unwanted elements like URLs, HTML tags, punctuation, numbers, and extra spaces were removed. Reviews with missing text were excluded.

3.  Custom Stopwords: A custom list of frequently repeated and non-informative words‚Äîsuch as "disney", "ride", "park", and location names‚Äîwas added to the stopword list.

4.  Tokenization and Filtering: Reviews were split into individual words (tokens), and all standard and custom stopwords were removed.

5.  Reconstruction: The remaining tokens were reassembled into cleaned review texts for each review ID.

6.  Merging: The cleaned text was merged back with the original dataset, preserving all metadata.

This process ensures that only the most meaningful and relevant words remain, improving the quality and accuracy of downstream text analysis.

```{r}

# Load stopwords
data("stop_words")

# remove symbols 
reviews_clean <- reviews %>%
  filter(!is.na(Review_Text)) %>%
  mutate(Review_ID,
         Review_Text = Review_Text %>%
           str_to_lower() %>%
           str_replace_all("http\\S+|www\\S+", "") %>%
           str_replace_all("<.*?>", "") %>%
           str_replace_all("[^a-z\\s]", " ") %>%
           str_squish()) 

custom_stops <- tibble(
  word = c("disney", "disneyland", "paris", "hong", "kong", "california", "ride", "rides", "park", "parks", "day")
)
all_stopwords <- bind_rows(stop_words, custom_stops)

# Tokenize and remove stopwords
tokens <- reviews_clean %>%
  unnest_tokens(word, Review_Text) %>%
  anti_join(all_stopwords, by = "word")

# Rebuild cleaned text from tokens and join back to full data
reconstructed <- tokens %>%
  group_by(Review_ID) %>%
  summarise(Review_Text_clean = str_c(word, collapse = " "), .groups = "drop")

# Join back to original metadata
reviews_clean <- reviews_clean %>%
  left_join(reconstructed, by = "Review_ID")

```

-   **Multilingual Reviews:**

Before performing text analysis, we first checked whether the reviews were written in multiple languages. Using a language detection function, the language of each review was identified based on its text content:

-   The `detect_language()` function was applied to the `Review_Text` column to assign a detected language code (e.g., "en" for English) to each review.

-   A filter was then applied to identify any reviews not written in English.

This step ensures that only English-language reviews are included in the analysis, avoiding noise or misinterpretation caused by multilingual content. Since its difficult to fully recognize other languages we will use multilingual models.

```{r}

# Detect language of each review
reviews$language <- detect_language(reviews$Review_Text)

# See if there are other languages in the data
reviews[reviews$language!= "en", ]


```

-   **Fake or Sponsored Reviews:**

To improve data quality and reduce the influence of spam or low-effort content, we implemented checks to identify and remove potentially fake or sponsored reviews. The filtering process involved two key steps:

1.  Short Review Removal:\
    Very short reviews (fewer than 10 words) were identified and excluded. These were predominantly 5-star ratings with minimal content, often lacking meaningful information for analysis.

2.  Duplicate Review Removal:\
    Reviews with identical cleaned text (`Review_Text_clean`) were flagged as duplicates, likely indicating copy-pasted or spammed content. Only unique reviews were retained by removing entries that appeared more than once.

This step helps ensure that the final dataset is composed of authentic, original, and content-rich reviews, enhancing the reliability of subsequent text analyses.

```{r}
# very short reviews seem to be mostly 5 star ratings
reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  arrange(word_count) %>%
  select(Rating, Review_Text, word_count) %>% 
  filter(word_count<10)

reviews_clean<-reviews_clean %>% 
  mutate(word_count = str_count(Review_Text, "\\w+")) %>%
  filter(word_count>10) %>% 
  group_by(Review_Text_clean) %>% 
    mutate(duplicate_count = n()) %>%
    ungroup() %>% 
    filter(duplicate_count == 1)


    
```

-   **Spelling Mistakes:**\

To solve this problem we mainly use NLPs.

--\> sie hat was dar√ºber gesagt, aber ich weiss es nicht mehr. M√ºssen wir das noch machen?

## part 1: Content analysis and topic modeling

üî∑ What can we tell about the customers that write reviews?

To begin our content analysis, we examined the length of review texts to understand how much customers typically write. The histogram above shows the distribution of review lengths (in number of characters).

The results reveal that:

-   Most customers write relatively short reviews, with a large concentration under 500 characters.

-   A long tail exists, indicating that while a few reviews are exceptionally long (over 10,000 characters), they are rare.

-   This suggests that the majority of feedback is brief, likely focused on high-level impressions or quick reactions, while only a minority of users provide detailed, in-depth reviews.

Understanding this behavior helps shape our expectations for topic modeling and sentiment analysis‚Äîbrief reviews may yield fewer unique insights per document, while longer reviews may offer richer semantic content.

```{r}
# most of the customers write shorter reviews
reviews %>%
  mutate(length = nchar(Review_Text)) %>%
  ggplot(aes(x = length)) +
  geom_histogram(binwidth = 20)

```

To understand the evolution of customer engagement over time, we visualized the number of reviews submitted each year across the three Disneyland branches: California, Hong Kong, and Paris.

The bar chart shows review counts by year and park:

-   California consistently leads in the number of reviews, peaking in 2015 with over 3,000 reviews.

-   Paris and Hong Kong also show increasing review activity until around 2015, with Paris slightly ahead of Hong Kong overall.

-   After 2016, all three parks experienced a gradual decline in review volume, which may be attributed to external factors like shifts in travel behavior, changing review habits, or global events.

```{r}

# Extract year
reviews$year <- year(reviews$Year)

# Plot: Reviews by year and branch
reviews %>%
  count(year, Branch) %>%
  ggplot(aes(x = year, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Year and Branch",
       x = "Year", y = "Number of Reviews") +
  theme_minimal()

```

To examine how review activity varies throughout the year, we analyzed the distribution of reviews by month for each Disneyland location: California, Hong Kong, and Paris.

The bar chart above displays the number of reviews by month:

-   Review activity increases during spring and summer, particularly from April to August, aligning with typical holiday and travel seasons.

-   July and August are peak months across all three branches, especially in California, suggesting these are high-traffic periods for park visitors.

-   There‚Äôs a secondary rise in November and December, possibly tied to winter holidays and seasonal events at the parks.

-   California receives the highest review volume consistently throughout the year, followed by Paris and then Hong Kong.

```{r}
# Extract month (abbreviated names for labels)
reviews$month <- month(reviews$Year_Month, label = TRUE, abbr = TRUE)

# Plot: Reviews by month and branch
reviews %>%
  count(month, Branch) %>%
  ggplot(aes(x = month, y = n, fill = Branch)) +
  geom_col(position = "dodge") +
  labs(title = "Review Counts by Month and Branch",
       x = "Month", y = "Number of Reviews") +
  theme_minimal()

```

To better understand who is writing the reviews, we categorized reviewers based on their location relative to each park:

-   Local: The reviewer is from the same country as the park.

-   Tourist: The reviewer is from a different country.

We matched each park with its corresponding country and compared it to the reviewer‚Äôs location to assign a Visitor_Type. The summary table shows the distribution of local and tourist reviewers for each Disneyland branch:

-   Disneyland Paris and Disneyland Hong Kong have overwhelmingly high proportions of tourist reviewers, with tourist ratios of 98.5% and 94.6%, respectively.

-   Disneyland California, by contrast, has a significantly higher share of local reviewers, with a tourist ratio of 36.5%.

This suggests that:

-   The Paris and Hong Kong parks attract primarily international visitors.

-   The California park serves a more balanced mix, with a strong local customer base likely due to its regional popularity and accessibility for U.S. residents.

Understanding the local-vs-tourist split is important for interpreting review content and tailoring park communication or services to different visitor types.

```{r}
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join to get park country
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  tidyr::pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(Total = Tourist + Local,
         Tourist_Ratio = Tourist / Total)

```

## Part 2: Sentiment analysis

üî∑ What do the visitors talk about in their reviews and how does it relate sentiment/ratings?

To understand the emotional tone of visitor reviews, we applied sentiment analysis using a pre-trained BERT model designed for multilingual text:\
Model: `nlptown/bert-base-multilingual-uncased-sentiment` (via HuggingFace Transformers)

#### Procedure:

1.  Sampling:\
    A random sample of 10,000 cleaned reviews was selected to ensure efficient yet representative processing.

2.  Truncation:\
    Each review was shortened to approximately 450 characters (\~512 tokens after tokenization) to meet model input constraints while retaining meaningful content.

3.  Sentiment Classification:\
    Using the HuggingFace Transformers pipeline, each review was passed through a sentiment classifier, which returned a predicted 1‚Äì5 star rating.

4.  Labeling:\
    The numeric star predictions were converted into sentiment labels:

    -   1‚Äì2 stars ‚Üí NEGATIVE

    -   3 stars ‚Üí NEUTRAL

    -   4‚Äì5 stars ‚Üí POSITIVE

This approach allowed us to classify review sentiment with a high degree of linguistic nuance, especially important for a multilingual dataset with short and informal text.

#### Why this matters:

This sentiment layer enables us to:

-   Compare emotional tone across parks and visitor types

-   Correlate sentiment with actual review ratings

-   Explore why Disneyland California may receive better reviews than Paris, especially considering its higher share of local visitors

Next, we will visualize and interpret these sentiment results to uncover key insights

```{r}
set.seed(9) 

# Shuffle and sample the data
sampled_reviews <- reviews_tagged %>%
  filter(!is.na(Review_Text_clean)) %>%
  slice_sample(n = 10000) %>% 
  mutate(Review_Text_clean = str_trunc(Review_Text_clean, 450))  # about 512 tokens after tokenization

# Load Python's transformers package
transformers <- import("transformers")

# Load sentiment analysis pipeline
sentiment_model <- transformers$pipeline(
  task = "text-classification",
  model = "nlptown/bert-base-multilingual-uncased-sentiment"
)


# Enable tokenizer parallelism 
Sys.setenv(TOKENIZERS_PARALLELISM = "true")

emo_results <- sampled_reviews %>%
  mutate(
    sentiment = map(Review_Text_clean, safely(sentiment_model))
  ) %>%
  mutate(
    sentiment_result = map(sentiment, ~ .x$result[[1]]),
    stars = map_dbl(sentiment_result, ~ as.numeric(str_extract(.x$label, "\\d"))),
    label = case_when(
      stars <= 2 ~ "NEGATIVE",
      stars == 3 ~ "NEUTRAL",
      stars >= 4 ~ "POSITIVE"
    )
  )

# Disable parallelism again     
Sys.setenv(TOKENIZERS_PARALLELISM = "false")


 
```

After applying sentiment analysis to a representative sample of reviews, we calculated the **average predicted sentiment score (1‚Äì5)** for each Disneyland branch:

```{r}

emo_results %>%
  group_by(Branch) %>%
  summarise(avg_sentiment = mean(Rating, na.rm = TRUE))

```

#### Key Observations:

-   Disneyland California has the highest average sentiment, suggesting visitors generally express more positive feelings in their reviews.

-   Disneyland Paris has the lowest sentiment score, indicating a less favorable emotional tone in its reviews, on average.

This supports the initial observation that California receives better reviews, and now we begin to understand how people feel, not just what star ratings they assign.

### Sentiment by Visitor Type and Park Branch

The chart above breaks down review sentiment by **visitor type (Local vs. Tourist)** for each Disneyland location. It reveals an important pattern:

```{r}

sentiment_by_visitor <- emo_results %>%
  group_by(Branch, Visitor_Type, label) %>%
  summarise(count = n(), .groups = "drop")


sentiment_normalized <- sentiment_by_visitor %>%
  group_by(Branch, Visitor_Type) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()


ggplot(sentiment_normalized, aes(x = Visitor_Type, y = percent, fill = label)) +
  geom_col(position = "dodge") +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment (%) by Visitor Type and Branch",
    x = "Visitor Type",
    y = "Percentage of Reviews",
    fill = "Sentiment"
  ) +
  theme_minimal()



```

#### Disneyland Paris:

-   Has the highest share of negative reviews, especially from tourists (over 40% negative).

-   Positive reviews from tourists are significantly lower than in California and Hong Kong.

-   Locals also express relatively low positivity and high negativity, though they represent a very small share of total reviewers.

#### Disneyland California:

-   Both locals and tourists show strong positive sentiment, with over 60% of reviews classified as positive.

-   Negative sentiment is notably lower among tourists here than in Paris.

#### Disneyland Hong Kong:

-   Sentiment is relatively balanced and positive for both locals and tourists.

-   Like California, over 60% of tourist reviews are positive

To better understand the content of negative feedback at Disneyland Paris, we generated a word cloud from the most frequently used words in negative sentiment reviews. Common branding terms like ‚ÄúDisneyland,‚Äù ‚ÄúDisney,‚Äù ‚Äúpark,‚Äù and ‚Äúday‚Äù were removed to focus on specific experiences.

```{r}

#| label: wordcloud

custom_stopwords <- c("disneyland", "disney", "park", "day")

# disneyland Paris
paris_tokens <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label=="NEGATIVE", !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean) %>%
  tokens(
    remove_punct = TRUE,
    remove_numbers = TRUE,
    remove_symbols = TRUE
  ) %>%
  tokens_tolower() %>%
  tokens_remove(pattern = custom_stopwords)


paris_dfm <- dfm(paris_tokens)
paris_dfm <- paris_dfm[ntoken(paris_dfm) > 0, ]

textplot_wordcloud(
  paris_dfm,
  min_count = 10,
  max_words = 100,
  color = "darkblue",
  min_size = 0.5,
  max_size = 4
)


```

#### Key Themes from the Word Cloud:

-   Time and waiting:\
    Words like *‚Äútime,‚Äù ‚Äúminutes,‚Äù ‚Äúqueue,‚Äù ‚Äúqueues,‚Äù ‚Äúwait,‚Äù ‚Äúhours,‚Äù ‚Äúwaiting,‚Äù ‚Äúlong,‚Äù ‚Äúclosed‚Äù* indicate visitor frustration with long wait times, ride closures, and inefficient time use.

-   Food and cost:\
    Words such as *‚Äúfood,‚Äù ‚Äúexpensive,‚Äù ‚Äúmoney,‚Äù ‚Äútickets,‚Äù ‚Äúcost‚Äù* suggest concerns about high prices and poor value for money.

-   Service and staff:\
    The prominence of *‚Äústaff,‚Äù ‚Äúrude,‚Äù ‚Äúservice,‚Äù ‚Äúhotel,‚Äù ‚Äúrestaurant‚Äù* points to inconsistent service quality and possible hospitality issues, especially in hotels and dining.

-   Children and families:\
    Frequent mentions of *‚Äúkids,‚Äù ‚Äúchildren,‚Äù ‚Äúfamily,‚Äù ‚Äúcharacters‚Äù* reflect expectations around family-friendly experiences‚Äîpossibly unmet for many visitors.

-   Crowds and space:\
    Words like *‚Äúpeople,‚Äù ‚Äúcrowded,‚Äù ‚Äúwalking,‚Äù ‚Äúline,‚Äù ‚Äúparade‚Äù* indicate overcrowding or logistical challenges in navigating the park.

### Conclusion:

This word cloud reinforces the findings from the sentiment analysis:

Many negative reviews for Disneyland Paris stem from long queues, high costs, and service quality, particularly from tourist visitors who may have high expectations and limited time.

Combined with the earlier insight that Paris has fewer local reviewers, it becomes clear that managing international guest expectations and experience logistics could be key to improving sentiment.

To go beyond individual words and uncover deeper patterns in the complaints, we applied BERTopic, a transformer-based topic modeling algorithm. This technique groups reviews into coherent topics based on semantic similarity, using a multilingual sentence embedding model to understand context and nuance across languages.

#### Model Setup:

-   Model: `BERTopic` with `paraphrase-multilingual-MiniLM-L12-v2` embedding

-   Data: 1,000 randomly sampled negative sentiment reviews from Disneyland Paris

-   Number of topics: Up to 6 (auto-reduced to 2 dominant topics based on content)

```{r}
# Load Python modules
bert <- import("bertopic")
sentence_transformers <- import("sentence_transformers")

# Embedding model (multilingual)
embedding_model <- sentence_transformers$SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

topic_model <- bert$BERTopic(
  embedding_model = embedding_model,
  nr_topics = r_to_py(6L),
  calculate_probabilities = TRUE,
  verbose = TRUE
)

docs <- emo_results %>%
  filter(Branch == "Disneyland_Paris", label == "NEGATIVE", !is.na(Review_Text_clean)) %>%
  slice_sample(n = 1000) %>%
  pull(Review_Text_clean)

# Fit model (run this in console, not inside Rmd chunk) it works for me though
topic_results <- topic_model$fit_transform(docs)

# Get summary of topics
topic_model$get_topic_info()






   
```

Key Takeaways:

-   One dominant topic (Topic 1) encompasses the vast majority of reviews, clearly indicating that logistical issues (e.g., waiting, food quality, staff performance) are the biggest source of dissatisfaction.

-   A smaller topic (Topic 0) highlights issues with cleanliness and family experience, which while less common, may point to isolated but impactful problems ‚Äî especially for visitors with children.

Contextual Insight

This analysis supports earlier findings from the word cloud and sentiment breakdown:

-   Tourists, who make up the vast majority of reviewers for Disneyland Paris, are particularly sensitive to time management, value for money, and service quality.

-   The lack of local reviewers means there‚Äôs less balancing feedback from visitors with lower expectations or more familiarity with the park experience.

Together, these topics provide actionable insight into why Disneyland Paris has a higher share of negative reviews, helping guide potential areas for improvement such as queue management, service training, and cleanliness.

Lets visulize

```{r}

topic_info <- topic_model$get_topic_info()
topic_info_df <- py_to_r(topic_info)




topic_info_df %>%
  filter(Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in Negative Disneyland Paris Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()


```

```{r}

embeddings <- as.data.frame(py_to_r(topic_model$umap_model$embedding_)) 

colnames(embeddings) <- c("UMAP1", "UMAP2")
topics <- reticulate::py_to_r(topic_results[[1]])
embeddings$topics <- as.factor(topics)
# Remove NA-named columns
embeddings <- embeddings[, !is.na(names(embeddings)) & names(embeddings) != ""]

# Now safe to filter
embeddings <- embeddings %>% filter(topics != -1)


# trimming of outliers
ggplotly(
  ggplot(embeddings) +
    geom_point(aes(UMAP1, UMAP2, color = topics)) +
    theme_classic()
)


```

now lets do the same thing for California positive since they have the most positive reviews in our sample.

```{r}


# Filter out text that appears more than once
emo_results <- emo_results %>%
  filter(duplicate_count == 1)

reviews_california_pos <- emo_results %>%
  filter(Branch == "Disneyland_California", label == "POSITIVE",  !is.na(Review_Text_clean)) %>%
  pull(Review_Text_clean)

reviews_california_pos <- reviews_california_pos %>%
  str_squish() %>%
  str_trunc(300)  # if input is long

# Use same embedding model as before
topic_model_california <- bert$BERTopic(
  embedding_model = embedding_model,
  min_topic_size = 30L,
  calculate_probabilities = TRUE,
  verbose = TRUE
)

topic_results_california <-topic_model_california$fit_transform(reviews_california_pos)
topic_model_california$get_topic_info()

topic_info_2 <- topic_model_california$get_topic_info()
topic_info_df_2 <- py_to_r(topic_info_2)




topic_info_df_2 %>%
  filter(Count >20, Topic!=-1) %>% 
  ggplot(aes(x = reorder(Name, Count), y = Count, fill = Name)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Frequency of Topics in positive California Reviews",
       x = "Theme", y = "Number of Reviews") +
  theme_minimal() +
  coord_flip()




```

based on these insights, paris could work on negative topics and offer more fastpass, shows and events like firework, special attractions, friendlier staff and cleaner toilets.

## Part 3: Locations analysis

üî∑ What differences can we detect for the three different locations and are there any interesting trends over time?

```{r}
reviews <- reviews %>%
  mutate(
    Year_Month = as_date(Year_Month),
    month = month(Year_Month, label = TRUE, abbr = TRUE),
    year = year(Year_Month)
  )

monthly_reviews <- reviews %>%
  filter(!is.na(month) & !is.na(year)) %>%
  count(year, month, Branch)

# Sort month correctly as a factor
monthly_reviews$month <- factor(monthly_reviews$month, 
                                 levels = month.abb, 
                                 ordered = TRUE)

# Line plot
ggplot(monthly_reviews, aes(x = month, y = n, color = as.factor(year), group = year)) +
  geom_line() +
  facet_wrap(~ Branch, scales = "free_y") +
  labs(title = "Monthly Review Trends per Year and Branch",
       x = "Month", y = "Number of Reviews", color = "Year") +
  theme_minimal()

```

```{r}
# Durchschnittliches Rating pro Jahr (und ggf. pro Standort)
reviews %>%
  group_by(Year, Branch) %>%
  summarise(avg_rating = mean(Rating, na.rm = TRUE)) %>%
  ggplot(aes(x = Year, y = avg_rating, color = Branch)) +
  geom_line(size = 1.2) +
  labs(title = "Average Rating Over the Years by Location",
       x = "Year",
       y = "Average Rating") +
  theme_minimal()


```

Sentimental

```{r}
sentiment_by_year <- emo_results %>%
  group_by(Branch, Year, label) %>%
  summarise(count = n(), .groups = "drop")

sentiment_normalized_yearly <- sentiment_by_year %>%
  group_by(Branch, Year) %>%
  mutate(percent = count / sum(count) * 100) %>%
  ungroup()

ggplot(sentiment_normalized_yearly, aes(x = Year, y = percent, color = label)) +
  geom_line(size = 1.2) +
  facet_wrap(~Branch) +
  labs(
    title = "Sentiment Trend Over the Years by Branch",
    x = "Year",
    y = "Percentage of Reviews",
    color = "Sentiment"
  ) +
  theme_minimal()
```

Visitor or Tourist:

```{r}
# Branch-country mapping
branch_country <- tibble::tibble(
  Branch = c("Disneyland_California", "Disneyland_Paris", "Disneyland_HongKong"),
  Country = c("United States", "France", "Hong Kong")
)

# Join and classify visitor type
reviews_tagged <- reviews_clean %>%
  left_join(branch_country, by = "Branch") %>%
  mutate(Visitor_Type = if_else(Reviewer_Location == Country, "Local", "Tourist"))

# Summarize and reshape data
summary_data <- reviews_tagged %>%
  count(Branch, Visitor_Type) %>%
  pivot_wider(names_from = Visitor_Type, values_from = n, values_fill = 0) %>%
  mutate(
    Total = Tourist + Local,
    Tourist_Ratio = Tourist / Total
  )

# Reshape for plotting
plot_data <- summary_data %>%
  pivot_longer(cols = c("Local", "Tourist"), names_to = "Visitor_Type", values_to = "Count")

# Plot
ggplot(plot_data, aes(x = Branch, y = Count, fill = Visitor_Type)) +
  geom_bar(stat = "identity") +
  geom_text(
    data = summary_data,
    aes(x = Branch, y = Total + 10, label = scales::percent(Tourist_Ratio, accuracy = 1)),
    inherit.aes = FALSE
  ) +
  labs(title = "Visitor Composition by Disneyland Branch",
       y = "Number of Visitors",
       x = "Branch") +
  theme_minimal() +
  scale_fill_manual(values = c("Local" = "yellow", "Tourist" = "#FF7415")) +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

```

## Part 4: Conclusion and Discussion

üî∑ What specific advice can we give to the Park management based on our analysis? How can we integrate the analysis of reviews in internal processes, can we think of any data products that would be of value?
